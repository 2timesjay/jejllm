{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS device not found. Using CUDA.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    torch_device = torch.device(\"mps:0\")\n",
    "    x = torch.ones(1, device=torch_device)\n",
    "    print (x)\n",
    "elif torch.cuda.is_available():\n",
    "    torch_device = torch.device(\"cuda\")\n",
    "    print (\"MPS device not found. Using CUDA.\")\n",
    "else:\n",
    "    torch_device = torch.device(\"cpu\")\n",
    "    print (\"No accelerator device found. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "EMBEDDING_DIM = 3\n",
    "CONTEXT_LENGTH = 4\n",
    "QKV_DIM = 2\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length=CONTEXT_LENGTH, dropout=0.5, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.w_q = nn.Linear(d_in, d_out, bias=qkv_bias).to(torch_device)\n",
    "        self.w_k = nn.Linear(d_in, d_out, bias=qkv_bias).to(torch_device)\n",
    "        self.w_v = nn.Linear(d_in, d_out, bias=qkv_bias).to(torch_device)\n",
    "        self.dropout = nn.Dropout(dropout).to(torch_device)\n",
    "        self.register_buffer(\n",
    "            'mask', \n",
    "            torch.triu(\n",
    "                torch.ones(context_length, context_length), \n",
    "                diagonal=1,\n",
    "            ).to(torch_device)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        num_tokens = x.shape[-2]\n",
    "        queries = self.w_q(x)\n",
    "        keys = self.w_k(x)\n",
    "        attn_scores = queries @ keys.transpose(-2, -1)\n",
    "        causal_attn_scores = attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        causal_attn_weights = torch.softmax(causal_attn_scores*(self.d_out**0.5), dim=-1)\n",
    "        causal_attn_weights = self.dropout(causal_attn_weights)\n",
    "        values = self.w_v(x)\n",
    "        context = causal_attn_weights @ values\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "EMBEDDING_DIM = 3\n",
    "CONTEXT_LENGTH = 4\n",
    "QKV_DIM = 2\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out//num_heads\n",
    "        self.w_q = nn.Linear(d_in, d_out, bias=qkv_bias).to(torch_device)\n",
    "        self.w_k = nn.Linear(d_in, d_out, bias=qkv_bias).to(torch_device)\n",
    "        self.w_v = nn.Linear(d_in, d_out, bias=qkv_bias).to(torch_device)\n",
    "        self.w_o = nn.Linear(d_out, d_out).to(torch_device)\n",
    "        # self.w_o = nn.Identity().to(torch_device)\n",
    "        self.dropout = nn.Dropout(dropout).to(torch_device)\n",
    "        self.register_buffer(\n",
    "            'mask', \n",
    "            torch.triu(\n",
    "                torch.ones(context_length, context_length), \n",
    "                diagonal=1,\n",
    "            ).to(torch_device)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.w_k(x)\n",
    "        queries = self.w_q(x)\n",
    "        values = self.w_v(x)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(-3, -2)\n",
    "        # TODO: KV Cache Optimization\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim).transpose(-3, -2)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim).transpose(-3, -2)\n",
    "        attn_scores = queries @ keys.transpose(-2, -1)\n",
    "        causal_attn_scores = attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        causal_attn_weights = torch.softmax(causal_attn_scores/(keys.shape[-1]**0.5), dim=-1)\n",
    "        causal_attn_weights = self.dropout(causal_attn_weights)\n",
    "        context = (causal_attn_weights @ values).transpose(-3, -2)\n",
    "        context = context.contiguous().view(b, num_tokens, self.num_heads*self.head_dim)\n",
    "        context = self.w_o(context)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(d_model))\n",
    "        self.shift = nn.Parameter(torch.zeros(d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True, unbiased=False)\n",
    "        x = (self.scale * (x - mean) / (std + self.eps)) + self.shift\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (\n",
    "            1 + torch.tanh(\n",
    "                torch.sqrt(torch.tensor(2.0/torch.pi, device=torch_device)) * (x  + 0.044715 * x**3)\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward (nn.Module):\n",
    "    def __init__(self, cfg): \n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], cfg[\"emb_dim\"]*4).to(torch_device),\n",
    "            GELU().to(torch_device),\n",
    "            nn.Linear(cfg[\"emb_dim\"]*4, cfg[\"emb_dim\"]).to(torch_device),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = LayerNorm(config[\"emb_dim\"]).to(torch_device)\n",
    "        self.attn = MultiHeadAttention(config[\"emb_dim\"], config[\"emb_dim\"], config[\"context_length\"], config[\"drop_rate\"], config[\"n_heads\"], config[\"qkv_bias\"]).to(torch_device)\n",
    "        self.drop = nn.Dropout(config[\"drop_rate\"]).to(torch_device)\n",
    "        self.ln2 = LayerNorm(config[\"emb_dim\"]).to(torch_device)\n",
    "        self.ff = FeedForward(config).to(torch_device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.drop(self.attn(self.ln1(x))) + x\n",
    "        x = self.drop(self.ff(self.ln2(x))) + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"]).to(torch_device)\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"]).to(torch_device)\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"]).to(torch_device)\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        ).to(torch_device)\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"]).to(torch_device)\n",
    "        # GPT2 uses tied weights for the embedding and output layers\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False).to(torch_device)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        in_idx = in_idx.to(torch_device)\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=torch_device))\n",
    "        x = self.drop_emb(tok_embeds + pos_embeds)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, token_ids, max_new_tokens, context_size):\n",
    "    logits = None\n",
    "    for i in range(max_new_tokens):\n",
    "        context_token_ids = token_ids[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(context_token_ids)\n",
    "        if isinstance(logits, tuple):\n",
    "            logits = logits.logits\n",
    "\n",
    "        logits = logits[:, -1, :]\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        token_id_next = torch.argmax(probas, dim=1, keepdim=True)  # Pure Greed\n",
    "        token_ids = torch.cat((token_ids, token_id_next), dim=1) \n",
    "\n",
    "    return token_ids\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special=set(['<|endoftext|>']))\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0).to(torch_device)\n",
    "    return encoded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logits, targets):\n",
    "    vocab_size = logits.shape[-1]\n",
    "    loss = nn.CrossEntropyLoss()(logits.view(-1, vocab_size), targets.view(-1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "#     def __init__(self, txt, tokenizer, max_length, stride):\n",
    "#         self.input_ids = []\n",
    "#         self.target_ids = []\n",
    "#         token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "#         token_ids = token_ids\n",
    "#         token_ids = torch.tensor(token_ids).to(torch_device)\n",
    "#         print(token_ids.shape)\n",
    "#         token_sequences = token_ids.unfold(0, max_length, stride)\n",
    "#         self.input_ids = token_sequences[:-1]\n",
    "#         self.target_ids = token_sequences[1:]\n",
    "\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "def create_dataloader_v1(\n",
    "        txt, batch_size=4, max_length=256, \n",
    "        stride=128, shuffle=True, drop_last=True,\n",
    "        num_workers=0,\n",
    "    ):\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    return DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device=torch_device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.view(-1, logits.shape[-1]), \n",
    "        target_batch.view(-1),\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(dataloader, model, device, num_batches=None):\n",
    "    total_loss = 0\n",
    "    if len(dataloader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(dataloader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(dataloader))\n",
    "    for i, (input_batch, target_batch) in enumerate(dataloader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_dataloader, test_dataloader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_dataloader, model, device, num_batches=eval_iter)\n",
    "        test_loss = calc_loss_loader(test_dataloader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, test_loss\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    token_ids = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(model, token_ids, 50, context_size)\n",
    "    decoded_text = tokenizer.decode(token_ids[0].tolist())\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(\n",
    "    model, train_dataloader, test_dataloader,\n",
    "    optimizer, device, num_epochs,\n",
    "    eval_freq, eval_iter, start_context, tokenizer\n",
    "):\n",
    "    train_losses, test_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            # print(input_batch)\n",
    "            # print(target_batch)\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            loss.backward()\n",
    "            # grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            # print(f\"Gradient norm: {grad_norm:.4f}\")\n",
    "            optimizer.step()\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step %eval_freq == 0:\n",
    "                train_loss, test_loss = evaluate_model(\n",
    "                    model, train_dataloader, test_dataloader, device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                test_losses.append(test_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(\n",
    "                    f\"Epoch {epoch + 1} (Step {global_step:06d}): \"\n",
    "                    f\"Train Loss: {train_loss:.3f}, \"\n",
    "                    f\"Val Loss: {test_loss:.3f}\"\n",
    "                )\n",
    "\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "    return train_losses, test_losses, track_tokens_seen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "train_ratio = 0.9\n",
    "split_idx = int(len(raw_text) * train_ratio)\n",
    "train_data = raw_text[:split_idx]\n",
    "test_data = raw_text[split_idx:]\n",
    "\n",
    "torch.manual_seed(123)\n",
    "train_dataloader = create_dataloader_v1(train_data, batch_size=2, max_length=GPT_CONFIG_124M[\"context_length\"], stride=GPT_CONFIG_124M[\"context_length\"], shuffle=True)\n",
    "test_dataloader = create_dataloader_v1(test_data, batch_size=2, max_length=GPT_CONFIG_124M[\"context_length\"], stride=GPT_CONFIG_124M[\"context_length\"], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data[:100], test_data[:100]\n",
    "# tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "# tokenizer.encode(train_data[:100], allowed_special={\"<|endoftext|>\"})\n",
    "# len(train_dataloader), len(test_dataloader)\n",
    "# input_sample, target_sample = next(iter(train_dataloader))\n",
    "# torch.sum(input_sample[1]), torch.sum(target_sample[1])\n",
    "# input_sample.shape, \n",
    "# input_sample[1],target_sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "gpt = GPTModel(GPT_CONFIG_124M).to(torch_device)#.to(torch.bfloat16)\n",
    "# gpt = torch.compile(gpt).to(torch_device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    gpt.parameters(),\n",
    "    lr = 0.0004, weight_decay = 0.1\n",
    ")\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Step 000000): Train Loss: 9.817, Val Loss: 9.928\n",
      "Epoch 1 (Step 000005): Train Loss: 8.065, Val Loss: 8.336\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Epoch 2 (Step 000010): Train Loss: 6.622, Val Loss: 7.052\n",
      "Epoch 2 (Step 000015): Train Loss: 6.047, Val Loss: 6.601\n",
      "Every effort moves you, and,, and,,,,,,, and,.                                   \n",
      "Epoch 3 (Step 000020): Train Loss: 5.587, Val Loss: 6.477\n",
      "Epoch 3 (Step 000025): Train Loss: 5.535, Val Loss: 6.404\n",
      "Every effort moves you, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and\n",
      "Epoch 4 (Step 000030): Train Loss: 5.157, Val Loss: 6.386\n",
      "Epoch 4 (Step 000035): Train Loss: 4.991, Val Loss: 6.385\n",
      "Every effort moves you a a so a a a. Gisburn, and a. Gisburn, and a, and a.            \"I the of the of the picture and he had been. I\n",
      "Epoch 5 (Step 000040): Train Loss: 4.366, Val Loss: 6.265\n",
      "Every effort moves you, I had been a--as of the--as of the of the of the, I had been--and it's had been, in the of the of the picture, as a of the of the of the man of the of the of\n",
      "Epoch 6 (Step 000045): Train Loss: 4.017, Val Loss: 6.201\n",
      "Epoch 6 (Step 000050): Train Loss: 3.511, Val Loss: 6.147\n",
      "Every effort moves you know the    \"I looked--I had a little.             \"I, and I had been the donkey.            \n",
      "Epoch 7 (Step 000055): Train Loss: 3.530, Val Loss: 6.175\n",
      "Epoch 7 (Step 000060): Train Loss: 2.720, Val Loss: 6.129\n",
      "Every effort moves you know the picture to see the picture.                    \"I he was his pictures-c.             \n",
      "Epoch 8 (Step 000065): Train Loss: 2.280, Val Loss: 6.143\n",
      "Epoch 8 (Step 000070): Train Loss: 1.936, Val Loss: 6.217\n",
      "Every effort moves you know,\" was not that the picture.  \"I had the last word. Gisburn's an!  \"Oh, in the moment--as Jack himself, as he was his own the donkey. \"There were days when I\n",
      "Epoch 9 (Step 000075): Train Loss: 1.559, Val Loss: 6.220\n",
      "Epoch 9 (Step 000080): Train Loss: 1.229, Val Loss: 6.245\n",
      "Every effort moves you know,\" was not that my hostess was \"interesting\": on that Mrs. \"Yes--and by me to me to have to see a smile behind his close that he had married her--the quality of Jack's \"strongest,\" she was\n",
      "Epoch 10 (Step 000085): Train Loss: 0.940, Val Loss: 6.315\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"     \"I didn't face that he had married her--the quality of the a fashionable painter--and by holding\n"
     ]
    }
   ],
   "source": [
    "train_losses, test_losses, track_tokens_seen = train_model_simple(\n",
    "    gpt, train_dataloader, test_dataloader, optimizer, torch_device,\n",
    "    num_epochs, eval_freq=5, eval_iter=5, start_context=\"Every effort moves you\", tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, test_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, test_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, track_tokens_seen, train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"     \"I didn't face that he had married her--the quality of the a fashionable painter--and by holding\n"
     ]
    }
   ],
   "source": [
    "generate_and_print_with_decoding(gpt, tokenizer, torch_device, \"Every effort moves you\", greedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_tokens(model, token_ids, max_new_tokens, context_size, top_k=5):\n",
    "    logits = None\n",
    "    for i in range(max_new_tokens):\n",
    "        context_token_ids = token_ids[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(context_token_ids)\n",
    "\n",
    "        logits = logits[:, -1, :]\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        token_id_next = torch.multinomial(probas, num_samples=1)  # Pure Random\n",
    "        token_ids = torch.cat((token_ids, token_id_next), dim=1) \n",
    "        # Print the top-k tokens\n",
    "        topk_probs, topk_tokens = torch.topk(probas, top_k, dim=-1)\n",
    "        for topk_prob, topk_token in zip(topk_probs[0], topk_tokens[0]):\n",
    "            token_str = tokenizer.decode([topk_token])\n",
    "            print(f\"{token_str}: {topk_prob:.4f}\")\n",
    "    \n",
    "    return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random(model, token_ids, max_new_tokens, context_size, temperature=1.0):\n",
    "    logits = None\n",
    "    if temperature == 0:\n",
    "        return greedy(model, token_ids, max_new_tokens, context_size)\n",
    "    for i in range(max_new_tokens):\n",
    "        context_token_ids = token_ids[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(context_token_ids)\n",
    "\n",
    "        logits = logits[:, -1, :]\n",
    "        probas = torch.softmax(logits/temperature, dim=-1)\n",
    "        token_id_next = torch.multinomial(probas, num_samples=1)  # Pure Random\n",
    "        token_ids = torch.cat((token_ids, token_id_next), dim=1) \n",
    "\n",
    "    return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\": 0.1868\n",
      " know: 0.1075\n",
      " in: 0.0496\n",
      " of: 0.0183\n",
      " sun: 0.0182\n",
      " terr: 0.0424\n",
      " to: 0.0277\n",
      " began: 0.0208\n",
      "\n",
      ": 0.0170\n",
      "agged: 0.0149\n",
      " him: 0.1311\n",
      "nt: 0.0391\n",
      " work: 0.0168\n",
      " never: 0.0152\n",
      " bitterness: 0.0146\n",
      " of: 0.0908\n",
      " up: 0.0900\n",
      " to: 0.0164\n",
      " with: 0.0164\n",
      ",: 0.0102\n",
      " it: 0.3579\n",
      " that: 0.1691\n",
      " a: 0.0327\n",
      " and: 0.0189\n",
      "-: 0.0130\n",
      " was: 0.1165\n",
      " happened: 0.0379\n",
      " all: 0.0323\n",
      " the: 0.0245\n",
      "?\": 0.0213\n",
      " one: 0.0809\n",
      " a: 0.0749\n",
      " Mrs: 0.0502\n",
      "ed: 0.0339\n",
      " dead: 0.0257\n",
      " he: 0.2288\n",
      " for: 0.0357\n",
      " by: 0.0220\n",
      " doesn: 0.0208\n",
      " ins: 0.0165\n",
      "--: 0.2171\n",
      " to: 0.2015\n",
      ".: 0.0157\n",
      " was: 0.0136\n",
      " after: 0.0118\n",
      "as: 0.0674\n",
      "I: 0.0672\n",
      "and: 0.0671\n",
      "his: 0.0561\n",
      "so: 0.0403\n",
      " the: 0.9259\n",
      " that: 0.0145\n",
      ",: 0.0091\n",
      " his: 0.0066\n",
      " a: 0.0011\n",
      " fact: 0.3842\n",
      " irony: 0.1359\n",
      " picture: 0.0216\n",
      " tone: 0.0161\n",
      "-: 0.0055\n",
      "chairs: 0.4696\n",
      "rooms: 0.0244\n",
      "hum: 0.0237\n",
      "piece: 0.0114\n",
      "century: 0.0110\n",
      " forward: 0.7530\n",
      ".: 0.0106\n",
      " his: 0.0080\n",
      " to: 0.0066\n",
      " me: 0.0041\n",
      ".: 0.9073\n",
      " of: 0.0195\n",
      " in: 0.0057\n",
      "anim: 0.0024\n",
      " was: 0.0012\n",
      " I: 0.2968\n",
      " \": 0.1636\n",
      "\n",
      ": 0.1337\n",
      " G: 0.0597\n",
      " The: 0.0354\n",
      " could: 0.2002\n",
      " had: 0.0688\n",
      " don: 0.0307\n",
      " didn: 0.0292\n",
      " so: 0.0271\n",
      " be: 0.0261\n",
      " that: 0.0256\n",
      "-: 0.0240\n",
      " her: 0.0208\n",
      " when: 0.0201\n",
      " given: 0.2962\n",
      " dis: 0.0534\n",
      " been: 0.0447\n",
      " my: 0.0283\n",
      " him: 0.0208\n",
      " to: 0.2538\n",
      " G: 0.0459\n",
      " no: 0.0195\n",
      " whole: 0.0163\n",
      "ident: 0.0116\n",
      " see: 0.1156\n",
      " me: 0.0902\n",
      " put: 0.0314\n",
      " the: 0.0265\n",
      " have: 0.0228\n",
      " of: 0.1683\n",
      " who: 0.0425\n",
      " me: 0.0372\n",
      ",: 0.0316\n",
      " about: 0.0131\n",
      " the: 0.2289\n",
      " my: 0.0481\n",
      " Jack: 0.0466\n",
      " anything: 0.0420\n",
      " his: 0.0351\n",
      "\n",
      ": 0.9371\n",
      " I: 0.0133\n",
      " The: 0.0037\n",
      " For: 0.0014\n",
      " She: 0.0013\n",
      "\n",
      ": 0.8965\n",
      "\": 0.0240\n",
      "I: 0.0148\n",
      "He: 0.0142\n",
      "His: 0.0052\n",
      " about: 0.0328\n",
      " seemed: 0.0264\n",
      " resolve: 0.0214\n",
      " had: 0.0202\n",
      "ing: 0.0162\n",
      " that: 0.1718\n",
      ".: 0.1126\n",
      " had: 0.0603\n",
      " up: 0.0301\n",
      "-: 0.0170\n",
      " him: 0.5683\n",
      " Mrs: 0.0949\n",
      " my: 0.0534\n",
      " that: 0.0394\n",
      " the: 0.0276\n",
      " of: 0.9190\n",
      " a: 0.0048\n",
      "--: 0.0047\n",
      " when: 0.0028\n",
      " like: 0.0024\n",
      " the: 0.9590\n",
      " her: 0.0028\n",
      ".\": 0.0017\n",
      " that: 0.0013\n",
      " his: 0.0013\n",
      " moment: 0.5564\n",
      " picture: 0.0188\n",
      " height: 0.0149\n",
      ";: 0.0094\n",
      " last: 0.0080\n",
      "--: 0.9051\n",
      " back: 0.0086\n",
      " in: 0.0067\n",
      " might: 0.0034\n",
      " I: 0.0033\n",
      " the: 0.1351\n",
      "as: 0.0274\n",
      " a: 0.0241\n",
      " his: 0.0196\n",
      " Mrs: 0.0174\n",
      " me: 0.3683\n",
      " Jack: 0.0482\n",
      " Mrs: 0.0476\n",
      " that: 0.0458\n",
      " his: 0.0418\n",
      ",: 0.3809\n",
      " to: 0.1566\n",
      "--: 0.0901\n",
      " with: 0.0340\n",
      " at: 0.0162\n",
      " look: 0.5322\n",
      " have: 0.0164\n",
      " see: 0.0129\n",
      " the: 0.0126\n",
      " put: 0.0118\n",
      ",: 0.1214\n",
      " him: 0.0820\n",
      " to: 0.0539\n",
      ".: 0.0491\n",
      " his: 0.0437\n",
      " his: 0.2174\n",
      ".: 0.1289\n",
      " at: 0.0345\n",
      " might: 0.0231\n",
      ";: 0.0217\n",
      ".: 0.8354\n",
      " of: 0.0293\n",
      "--: 0.0270\n",
      ",: 0.0099\n",
      " and: 0.0036\n",
      " I: 0.8135\n",
      " But: 0.0169\n",
      " \": 0.0115\n",
      "\n",
      ": 0.0103\n",
      " Rick: 0.0079\n",
      " had: 0.1923\n",
      " saw: 0.1068\n",
      ",: 0.0543\n",
      " may: 0.0473\n",
      " couldn: 0.0239\n",
      " of: 0.4007\n",
      " _: 0.0209\n",
      " up: 0.0191\n",
      " from: 0.0175\n",
      " wander: 0.0115\n",
      " modesty: 0.8106\n",
      " up: 0.0097\n",
      " by: 0.0027\n",
      "-: 0.0027\n",
      " _: 0.0015\n",
      ",: 0.8726\n",
      "--: 0.0201\n",
      " about: 0.0189\n",
      " when: 0.0043\n",
      " of: 0.0035\n",
      " and: 0.4206\n",
      " with: 0.0409\n",
      " you: 0.0370\n",
      " as: 0.0140\n",
      " on: 0.0114\n",
      " were: 0.0453\n",
      " heard: 0.0195\n",
      "in: 0.0149\n",
      "itation: 0.0147\n",
      " that: 0.0094\n",
      ",: 0.7076\n",
      " a: 0.0152\n",
      " were: 0.0078\n",
      " on: 0.0073\n",
      " room: 0.0066\n",
      "I: 0.2563\n",
      "\n",
      ": 0.0220\n",
      ",\": 0.0216\n",
      " himself: 0.0211\n",
      " G: 0.0130\n",
      ",: 0.0971\n",
      "\": 0.0550\n",
      "--: 0.0533\n",
      " when: 0.0436\n",
      " by: 0.0376\n",
      "I: 0.2801\n",
      "Oh: 0.0755\n",
      "Well: 0.0199\n",
      "Ah: 0.0155\n",
      "\n",
      ": 0.0131\n",
      "Every effort moves youitationseen trace put it was other beard--all the-chairs forward. I marble have only to money of!\"  A alternate on appearance of the momentuscript told me to Tab between frames. I louder ridiculous modesty,ig heard (ly\"that\n"
     ]
    }
   ],
   "source": [
    "generate_and_print_with_decoding(gpt, tokenizer, torch_device, \"Every effort moves you\", print_top_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"   He laughed of the moment--as Jack himself, and he was have him done by a fashionable painter--and by holding\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "generate_and_print_with_decoding(gpt, tokenizer, torch_device, \"Every effort moves you\", partial(random, temperature=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = torch.rand((10, 10))\n",
    "# vals, inds = torch.topk(temp, dim=-1, k=3)\n",
    "# mask = torch.ones_like(temp, dtype=torch.bool).scatter_(-1, inds, False)\n",
    "# print(mask)\n",
    "# topk_temp = temp.masked_fill_(mask, -torch.inf)\n",
    "# torch.softmax(topk_temp, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. \"Oh, with a yourself comfortable--and here are the cigars you like.\"  I looked at the donkey--and I had the donkey. \"There were days when I\n"
     ]
    }
   ],
   "source": [
    "generate_and_print_with_decoding(gpt, tokenizer, torch_device, \"Every effort moves you\", partial(topk, temperature=0.5, k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minp(model, token_ids, max_new_tokens, context_size, temperature=1.0, minp=0.9):\n",
    "    logits = None\n",
    "    if temperature == 0:\n",
    "        return greedy(model, token_ids, max_new_tokens, context_size)\n",
    "    for i in range(max_new_tokens):\n",
    "        context_token_ids = token_ids[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(context_token_ids)\n",
    "\n",
    "        logits = logits[:, -1, :]\n",
    "        probas = torch.softmax(logits/temperature, dim=-1)\n",
    "        sorted_probas, sorted_indices = torch.sort(probas, descending=True, dim=-1)\n",
    "        cumulative_probas = torch.cumsum(sorted_probas, dim=-1)\n",
    "        minp_mask = cumulative_probas > minp\n",
    "        unsorted_minp_mask = torch.zeros_like(probas, dtype=torch.bool).scatter_(-1, sorted_indices, minp_mask)\n",
    "        sum_prob = torch.sum(sorted_probas.masked_fill_(minp_mask, 0), dim=-1)\n",
    "        if sum_prob == 0:\n",
    "            token_id_next = torch.argmax(probas, dim=-1, keepdim=True)  # Pure Greed\n",
    "            token_ids = torch.cat((token_ids, token_id_next), dim=1) \n",
    "        else:\n",
    "            minp_probas = probas.masked_fill_(unsorted_minp_mask, 0)/sum_prob\n",
    "            token_id_next = torch.multinomial(minp_probas, num_samples=1)  # Pure Random\n",
    "            token_ids = torch.cat((token_ids, token_id_next), dim=1) \n",
    "\n",
    "    return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you know,\" was one of the window. The she proveujエ for gar wasn even he was hint of an awful simpleton, that, I don Stroud smile canv to deard Frost the sketch of the donkey. \"There were days when I\n"
     ]
    }
   ],
   "source": [
    "generate_and_print_with_decoding(gpt, tokenizer, torch_device, \"Every effort moves you\", partial(minp, temperature=1.0, minp=0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gpt.state_dict(), \"gpt-trained.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16969/3928202563.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_gpt_state = torch.load(\"gpt-trained.pth\", map_location=torch_device)\n"
     ]
    }
   ],
   "source": [
    "loaded_gpt_state = torch.load(\"gpt-trained.pth\", map_location=torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_gpt_model = GPTModel(GPT_CONFIG_124M).to(torch_device)\n",
    "loaded_gpt_model.load_state_dict(loaded_gpt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_gpt_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled = torch.compile(loaded_gpt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"     \"I didn't face that he had married her--the quality of the a fashionable painter--and by holding\n"
     ]
    }
   ],
   "source": [
    "generate_and_print_with_decoding(compiled, tokenizer, torch_device, \"Every effort moves you\", greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading from HF Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacobjensen/.pyenv/versions/3.11.9/envs/jejllm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "# Load the model\n",
    "hf_model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "# Extract the weights\n",
    "hf_model_state_dict = hf_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename(s):\n",
    "    return s.replace(\n",
    "        \"transformer\", \"trf\"\n",
    "    ).replace(\n",
    "        \"h.\", \"\"\n",
    "    ).replace(\n",
    "        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight torch.Size([50257, 768])\n",
      "transformer.wpe.weight torch.Size([1024, 768])\n",
      "transformer.h.0.ln_1.weight torch.Size([768])\n",
      "transformer.h.0.ln_1.bias torch.Size([768])\n",
      "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.0.ln_2.weight torch.Size([768])\n",
      "transformer.h.0.ln_2.bias torch.Size([768])\n",
      "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_1.weight torch.Size([768])\n",
      "transformer.h.1.ln_1.bias torch.Size([768])\n",
      "transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.1.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.1.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_2.weight torch.Size([768])\n",
      "transformer.h.1.ln_2.bias torch.Size([768])\n",
      "transformer.h.1.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.1.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.1.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.1.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_1.weight torch.Size([768])\n",
      "transformer.h.2.ln_1.bias torch.Size([768])\n",
      "transformer.h.2.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.2.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.2.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.2.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_2.weight torch.Size([768])\n",
      "transformer.h.2.ln_2.bias torch.Size([768])\n",
      "transformer.h.2.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.2.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.2.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.2.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_1.weight torch.Size([768])\n",
      "transformer.h.3.ln_1.bias torch.Size([768])\n",
      "transformer.h.3.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.3.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.3.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.3.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_2.weight torch.Size([768])\n",
      "transformer.h.3.ln_2.bias torch.Size([768])\n",
      "transformer.h.3.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.3.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.3.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.3.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_1.weight torch.Size([768])\n",
      "transformer.h.4.ln_1.bias torch.Size([768])\n",
      "transformer.h.4.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.4.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.4.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.4.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_2.weight torch.Size([768])\n",
      "transformer.h.4.ln_2.bias torch.Size([768])\n",
      "transformer.h.4.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.4.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.4.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.4.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_1.weight torch.Size([768])\n",
      "transformer.h.5.ln_1.bias torch.Size([768])\n",
      "transformer.h.5.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.5.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.5.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.5.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_2.weight torch.Size([768])\n",
      "transformer.h.5.ln_2.bias torch.Size([768])\n",
      "transformer.h.5.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.5.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.5.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.5.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_1.weight torch.Size([768])\n",
      "transformer.h.6.ln_1.bias torch.Size([768])\n",
      "transformer.h.6.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.6.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.6.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.6.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_2.weight torch.Size([768])\n",
      "transformer.h.6.ln_2.bias torch.Size([768])\n",
      "transformer.h.6.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.6.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.6.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.6.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_1.weight torch.Size([768])\n",
      "transformer.h.7.ln_1.bias torch.Size([768])\n",
      "transformer.h.7.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.7.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.7.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.7.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_2.weight torch.Size([768])\n",
      "transformer.h.7.ln_2.bias torch.Size([768])\n",
      "transformer.h.7.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.7.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.7.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.7.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_1.weight torch.Size([768])\n",
      "transformer.h.8.ln_1.bias torch.Size([768])\n",
      "transformer.h.8.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.8.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.8.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.8.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_2.weight torch.Size([768])\n",
      "transformer.h.8.ln_2.bias torch.Size([768])\n",
      "transformer.h.8.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.8.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.8.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.8.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_1.weight torch.Size([768])\n",
      "transformer.h.9.ln_1.bias torch.Size([768])\n",
      "transformer.h.9.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.9.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.9.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.9.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_2.weight torch.Size([768])\n",
      "transformer.h.9.ln_2.bias torch.Size([768])\n",
      "transformer.h.9.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.9.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.9.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.9.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_1.weight torch.Size([768])\n",
      "transformer.h.10.ln_1.bias torch.Size([768])\n",
      "transformer.h.10.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.10.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.10.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.10.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_2.weight torch.Size([768])\n",
      "transformer.h.10.ln_2.bias torch.Size([768])\n",
      "transformer.h.10.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.10.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.10.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.10.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_1.weight torch.Size([768])\n",
      "transformer.h.11.ln_1.bias torch.Size([768])\n",
      "transformer.h.11.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.11.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.11.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.11.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_2.weight torch.Size([768])\n",
      "transformer.h.11.ln_2.bias torch.Size([768])\n",
      "transformer.h.11.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.11.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.11.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.11.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.ln_f.weight torch.Size([768])\n",
      "transformer.ln_f.bias torch.Size([768])\n",
      "lm_head.weight torch.Size([50257, 768])\n",
      "124439808 163037184\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in hf_model.parameters())\n",
    "total_params_from_state = 0\n",
    "for k, v in hf_model_state_dict.items():\n",
    "    print(k, v.shape)\n",
    "    total_params_from_state += v.numel()\n",
    "print(total_params, total_params_from_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M_ORIGINAL = GPT_CONFIG_124M.copy()\n",
    "GPT_CONFIG_124M_ORIGINAL[\"qkv_bias\"] = True\n",
    "GPT_CONFIG_124M_ORIGINAL[\"context_length\"] = 1024\n",
    "gpt2_small = GPTModel(GPT_CONFIG_124M_ORIGINAL).to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 50257,\n",
       " 'context_length': 1024,\n",
       " 'emb_dim': 768,\n",
       " 'n_heads': 12,\n",
       " 'n_layers': 12,\n",
       " 'drop_rate': 0.1,\n",
       " 'qkv_bias': True}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT_CONFIG_124M_ORIGINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_emb.weight torch.Size([50257, 768])\n",
      "pos_emb.weight torch.Size([1024, 768])\n",
      "trf_blocks.0.ln1.scale torch.Size([768])\n",
      "trf_blocks.0.ln1.shift torch.Size([768])\n",
      "trf_blocks.0.attn.mask torch.Size([1024, 1024])\n",
      "trf_blocks.0.attn.w_q.weight torch.Size([768, 768])\n",
      "trf_blocks.0.attn.w_q.bias torch.Size([768])\n",
      "trf_blocks.0.attn.w_k.weight torch.Size([768, 768])\n",
      "trf_blocks.0.attn.w_k.bias torch.Size([768])\n",
      "trf_blocks.0.attn.w_v.weight torch.Size([768, 768])\n",
      "trf_blocks.0.attn.w_v.bias torch.Size([768])\n",
      "trf_blocks.0.attn.w_o.weight torch.Size([768, 768])\n",
      "trf_blocks.0.attn.w_o.bias torch.Size([768])\n",
      "trf_blocks.0.ln2.scale torch.Size([768])\n",
      "trf_blocks.0.ln2.shift torch.Size([768])\n",
      "trf_blocks.0.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.0.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.0.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.0.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.1.ln1.scale torch.Size([768])\n",
      "trf_blocks.1.ln1.shift torch.Size([768])\n",
      "trf_blocks.1.attn.mask torch.Size([1024, 1024])\n",
      "trf_blocks.1.attn.w_q.weight torch.Size([768, 768])\n",
      "trf_blocks.1.attn.w_q.bias torch.Size([768])\n",
      "trf_blocks.1.attn.w_k.weight torch.Size([768, 768])\n",
      "trf_blocks.1.attn.w_k.bias torch.Size([768])\n",
      "trf_blocks.1.attn.w_v.weight torch.Size([768, 768])\n",
      "trf_blocks.1.attn.w_v.bias torch.Size([768])\n",
      "trf_blocks.1.attn.w_o.weight torch.Size([768, 768])\n",
      "trf_blocks.1.attn.w_o.bias torch.Size([768])\n",
      "trf_blocks.1.ln2.scale torch.Size([768])\n",
      "trf_blocks.1.ln2.shift torch.Size([768])\n",
      "trf_blocks.1.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.1.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.1.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.1.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.2.ln1.scale torch.Size([768])\n",
      "trf_blocks.2.ln1.shift torch.Size([768])\n",
      "trf_blocks.2.attn.mask torch.Size([1024, 1024])\n",
      "trf_blocks.2.attn.w_q.weight torch.Size([768, 768])\n",
      "trf_blocks.2.attn.w_q.bias torch.Size([768])\n",
      "trf_blocks.2.attn.w_k.weight torch.Size([768, 768])\n",
      "trf_blocks.2.attn.w_k.bias torch.Size([768])\n",
      "trf_blocks.2.attn.w_v.weight torch.Size([768, 768])\n",
      "trf_blocks.2.attn.w_v.bias torch.Size([768])\n",
      "trf_blocks.2.attn.w_o.weight torch.Size([768, 768])\n",
      "trf_blocks.2.attn.w_o.bias torch.Size([768])\n",
      "trf_blocks.2.ln2.scale torch.Size([768])\n",
      "trf_blocks.2.ln2.shift torch.Size([768])\n",
      "trf_blocks.2.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.2.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.2.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.2.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.3.ln1.scale torch.Size([768])\n",
      "trf_blocks.3.ln1.shift torch.Size([768])\n",
      "trf_blocks.3.attn.mask torch.Size([1024, 1024])\n",
      "trf_blocks.3.attn.w_q.weight torch.Size([768, 768])\n",
      "trf_blocks.3.attn.w_q.bias torch.Size([768])\n",
      "trf_blocks.3.attn.w_k.weight torch.Size([768, 768])\n",
      "trf_blocks.3.attn.w_k.bias torch.Size([768])\n",
      "trf_blocks.3.attn.w_v.weight torch.Size([768, 768])\n",
      "trf_blocks.3.attn.w_v.bias torch.Size([768])\n",
      "trf_blocks.3.attn.w_o.weight torch.Size([768, 768])\n",
      "trf_blocks.3.attn.w_o.bias torch.Size([768])\n",
      "trf_blocks.3.ln2.scale torch.Size([768])\n",
      "trf_blocks.3.ln2.shift torch.Size([768])\n",
      "trf_blocks.3.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.3.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.3.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.3.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.4.ln1.scale torch.Size([768])\n",
      "trf_blocks.4.ln1.shift torch.Size([768])\n",
      "trf_blocks.4.attn.mask torch.Size([1024, 1024])\n",
      "trf_blocks.4.attn.w_q.weight torch.Size([768, 768])\n",
      "trf_blocks.4.attn.w_q.bias torch.Size([768])\n",
      "trf_blocks.4.attn.w_k.weight torch.Size([768, 768])\n",
      "trf_blocks.4.attn.w_k.bias torch.Size([768])\n",
      "trf_blocks.4.attn.w_v.weight torch.Size([768, 768])\n",
      "trf_blocks.4.attn.w_v.bias torch.Size([768])\n",
      "trf_blocks.4.attn.w_o.weight torch.Size([768, 768])\n",
      "trf_blocks.4.attn.w_o.bias torch.Size([768])\n",
      "trf_blocks.4.ln2.scale torch.Size([768])\n",
      "trf_blocks.4.ln2.shift torch.Size([768])\n",
      "trf_blocks.4.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.4.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.4.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.4.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.5.ln1.scale torch.Size([768])\n",
      "trf_blocks.5.ln1.shift torch.Size([768])\n",
      "trf_blocks.5.attn.mask torch.Size([1024, 1024])\n",
      "trf_blocks.5.attn.w_q.weight torch.Size([768, 768])\n",
      "trf_blocks.5.attn.w_q.bias torch.Size([768])\n",
      "trf_blocks.5.attn.w_k.weight torch.Size([768, 768])\n",
      "trf_blocks.5.attn.w_k.bias torch.Size([768])\n",
      "trf_blocks.5.attn.w_v.weight torch.Size([768, 768])\n",
      "trf_blocks.5.attn.w_v.bias torch.Size([768])\n",
      "trf_blocks.5.attn.w_o.weight torch.Size([768, 768])\n",
      "trf_blocks.5.attn.w_o.bias torch.Size([768])\n",
      "trf_blocks.5.ln2.scale torch.Size([768])\n",
      "trf_blocks.5.ln2.shift torch.Size([768])\n",
      "trf_blocks.5.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.5.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.5.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.5.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.6.ln1.scale torch.Size([768])\n",
      "trf_blocks.6.ln1.shift torch.Size([768])\n",
      "trf_blocks.6.attn.mask torch.Size([1024, 1024])\n",
      "trf_blocks.6.attn.w_q.weight torch.Size([768, 768])\n",
      "trf_blocks.6.attn.w_q.bias torch.Size([768])\n",
      "trf_blocks.6.attn.w_k.weight torch.Size([768, 768])\n",
      "trf_blocks.6.attn.w_k.bias torch.Size([768])\n",
      "trf_blocks.6.attn.w_v.weight torch.Size([768, 768])\n",
      "trf_blocks.6.attn.w_v.bias torch.Size([768])\n",
      "trf_blocks.6.attn.w_o.weight torch.Size([768, 768])\n",
      "trf_blocks.6.attn.w_o.bias torch.Size([768])\n",
      "trf_blocks.6.ln2.scale torch.Size([768])\n",
      "trf_blocks.6.ln2.shift torch.Size([768])\n",
      "trf_blocks.6.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.6.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.6.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.6.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.7.ln1.scale torch.Size([768])\n",
      "trf_blocks.7.ln1.shift torch.Size([768])\n",
      "trf_blocks.7.attn.mask torch.Size([1024, 1024])\n",
      "trf_blocks.7.attn.w_q.weight torch.Size([768, 768])\n",
      "trf_blocks.7.attn.w_q.bias torch.Size([768])\n",
      "trf_blocks.7.attn.w_k.weight torch.Size([768, 768])\n",
      "trf_blocks.7.attn.w_k.bias torch.Size([768])\n",
      "trf_blocks.7.attn.w_v.weight torch.Size([768, 768])\n",
      "trf_blocks.7.attn.w_v.bias torch.Size([768])\n",
      "trf_blocks.7.attn.w_o.weight torch.Size([768, 768])\n",
      "trf_blocks.7.attn.w_o.bias torch.Size([768])\n",
      "trf_blocks.7.ln2.scale torch.Size([768])\n",
      "trf_blocks.7.ln2.shift torch.Size([768])\n",
      "trf_blocks.7.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.7.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.7.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.7.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.8.ln1.scale torch.Size([768])\n",
      "trf_blocks.8.ln1.shift torch.Size([768])\n",
      "trf_blocks.8.attn.mask torch.Size([1024, 1024])\n",
      "trf_blocks.8.attn.w_q.weight torch.Size([768, 768])\n",
      "trf_blocks.8.attn.w_q.bias torch.Size([768])\n",
      "trf_blocks.8.attn.w_k.weight torch.Size([768, 768])\n",
      "trf_blocks.8.attn.w_k.bias torch.Size([768])\n",
      "trf_blocks.8.attn.w_v.weight torch.Size([768, 768])\n",
      "trf_blocks.8.attn.w_v.bias torch.Size([768])\n",
      "trf_blocks.8.attn.w_o.weight torch.Size([768, 768])\n",
      "trf_blocks.8.attn.w_o.bias torch.Size([768])\n",
      "trf_blocks.8.ln2.scale torch.Size([768])\n",
      "trf_blocks.8.ln2.shift torch.Size([768])\n",
      "trf_blocks.8.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.8.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.8.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.8.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.9.ln1.scale torch.Size([768])\n",
      "trf_blocks.9.ln1.shift torch.Size([768])\n",
      "trf_blocks.9.attn.mask torch.Size([1024, 1024])\n",
      "trf_blocks.9.attn.w_q.weight torch.Size([768, 768])\n",
      "trf_blocks.9.attn.w_q.bias torch.Size([768])\n",
      "trf_blocks.9.attn.w_k.weight torch.Size([768, 768])\n",
      "trf_blocks.9.attn.w_k.bias torch.Size([768])\n",
      "trf_blocks.9.attn.w_v.weight torch.Size([768, 768])\n",
      "trf_blocks.9.attn.w_v.bias torch.Size([768])\n",
      "trf_blocks.9.attn.w_o.weight torch.Size([768, 768])\n",
      "trf_blocks.9.attn.w_o.bias torch.Size([768])\n",
      "trf_blocks.9.ln2.scale torch.Size([768])\n",
      "trf_blocks.9.ln2.shift torch.Size([768])\n",
      "trf_blocks.9.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.9.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.9.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.9.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.10.ln1.scale torch.Size([768])\n",
      "trf_blocks.10.ln1.shift torch.Size([768])\n",
      "trf_blocks.10.attn.mask torch.Size([1024, 1024])\n",
      "trf_blocks.10.attn.w_q.weight torch.Size([768, 768])\n",
      "trf_blocks.10.attn.w_q.bias torch.Size([768])\n",
      "trf_blocks.10.attn.w_k.weight torch.Size([768, 768])\n",
      "trf_blocks.10.attn.w_k.bias torch.Size([768])\n",
      "trf_blocks.10.attn.w_v.weight torch.Size([768, 768])\n",
      "trf_blocks.10.attn.w_v.bias torch.Size([768])\n",
      "trf_blocks.10.attn.w_o.weight torch.Size([768, 768])\n",
      "trf_blocks.10.attn.w_o.bias torch.Size([768])\n",
      "trf_blocks.10.ln2.scale torch.Size([768])\n",
      "trf_blocks.10.ln2.shift torch.Size([768])\n",
      "trf_blocks.10.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.10.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.10.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.10.ff.layers.2.bias torch.Size([768])\n",
      "trf_blocks.11.ln1.scale torch.Size([768])\n",
      "trf_blocks.11.ln1.shift torch.Size([768])\n",
      "trf_blocks.11.attn.mask torch.Size([1024, 1024])\n",
      "trf_blocks.11.attn.w_q.weight torch.Size([768, 768])\n",
      "trf_blocks.11.attn.w_q.bias torch.Size([768])\n",
      "trf_blocks.11.attn.w_k.weight torch.Size([768, 768])\n",
      "trf_blocks.11.attn.w_k.bias torch.Size([768])\n",
      "trf_blocks.11.attn.w_v.weight torch.Size([768, 768])\n",
      "trf_blocks.11.attn.w_v.bias torch.Size([768])\n",
      "trf_blocks.11.attn.w_o.weight torch.Size([768, 768])\n",
      "trf_blocks.11.attn.w_o.bias torch.Size([768])\n",
      "trf_blocks.11.ln2.scale torch.Size([768])\n",
      "trf_blocks.11.ln2.shift torch.Size([768])\n",
      "trf_blocks.11.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.11.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.11.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.11.ff.layers.2.bias torch.Size([768])\n",
      "final_norm.scale torch.Size([768])\n",
      "final_norm.shift torch.Size([768])\n",
      "out_head.weight torch.Size([50257, 768])\n",
      "124439808 338657280\n"
     ]
    }
   ],
   "source": [
    "# for p in gpt2_small.parameters():\n",
    "#     print(p)\n",
    "for k, v in gpt2_small.state_dict().items():\n",
    "    print(k, v.shape)\n",
    "    total_params_from_state += v.numel()\n",
    "print(total_params, total_params_from_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def convert_name(old_name, tensor):\n",
    "    \"\"\"\n",
    "    Convert parameter names from the initial naming convention to the target naming convention.\n",
    "\n",
    "    Args:\n",
    "        old_name (str): The original parameter name.\n",
    "        tensor (torch.Tensor): The tensor associated with the parameter name.\n",
    "\n",
    "    Returns:\n",
    "        list of tuples: Each tuple contains the new parameter name and the corresponding slice indices\n",
    "                        if the old parameter needs to be split. Otherwise, slice indices are None.\n",
    "                        Example: [(\"trf_blocks.0.attn.w_q.weight\", slice(0, 768))]\n",
    "    \"\"\"\n",
    "    mappings = []\n",
    "\n",
    "    # Direct mappings\n",
    "    direct_map = {\n",
    "        \"transformer.wte.weight\": \"tok_emb.weight\",\n",
    "        \"transformer.wpe.weight\": \"pos_emb.weight\",\n",
    "        \"transformer.ln_f.weight\": \"final_norm.scale\",\n",
    "        \"transformer.ln_f.bias\": \"final_norm.shift\",\n",
    "        \"lm_head.weight\": \"out_head.weight\",\n",
    "    }\n",
    "\n",
    "    if old_name in direct_map:\n",
    "        mappings.append((direct_map[old_name], None))\n",
    "        return mappings\n",
    "\n",
    "    # Regex patterns for more complex mappings\n",
    "    layer_pattern = re.compile(r\"transformer\\.h\\.(\\d+)\\.(ln_1|ln_2|attn\\.(c_attn|c_proj)|mlp\\.(c_fc|c_proj))\\.(weight|bias)\")\n",
    "\n",
    "    match = layer_pattern.match(old_name)\n",
    "    if match:\n",
    "        layer_idx, component, sub_component, mlp_component, param_type = match.groups()\n",
    "        layer_idx = int(layer_idx)\n",
    "\n",
    "        if component == \"ln_1\":\n",
    "            if param_type == \"weight\":\n",
    "                new_name = f\"trf_blocks.{layer_idx}.ln1.scale\"\n",
    "            else:\n",
    "                new_name = f\"trf_blocks.{layer_idx}.ln1.shift\"\n",
    "            mappings.append((new_name, None))\n",
    "        elif component == \"ln_2\":\n",
    "            if param_type == \"weight\":\n",
    "                new_name = f\"trf_blocks.{layer_idx}.ln2.scale\"\n",
    "            else:\n",
    "                new_name = f\"trf_blocks.{layer_idx}.ln2.shift\"\n",
    "            mappings.append((new_name, None))\n",
    "        elif component.startswith(\"attn.c_attn\"):\n",
    "            if param_type == \"weight\":\n",
    "                # Split c_attn.weight into w_q.weight, w_k.weight, w_v.weight\n",
    "                d_model = tensor.size(1)  # Assuming [768, 2304]\n",
    "                split_size = d_model // 3\n",
    "                w_q = f\"trf_blocks.{layer_idx}.attn.w_q.weight\"\n",
    "                w_k = f\"trf_blocks.{layer_idx}.attn.w_k.weight\"\n",
    "                w_v = f\"trf_blocks.{layer_idx}.attn.w_v.weight\"\n",
    "                mappings.extend([\n",
    "                    (w_q, slice(0, split_size)),\n",
    "                    (w_k, slice(split_size, 2 * split_size)),\n",
    "                    (w_v, slice(2 * split_size, 3 * split_size)),\n",
    "                ])\n",
    "            elif param_type == \"bias\":\n",
    "                d_model = tensor.size(0)\n",
    "                split_size = d_model // 3\n",
    "                b_q = f\"trf_blocks.{layer_idx}.attn.w_q.bias\"\n",
    "                b_k = f\"trf_blocks.{layer_idx}.attn.w_k.bias\"\n",
    "                b_v = f\"trf_blocks.{layer_idx}.attn.w_v.bias\"\n",
    "                mappings.extend([\n",
    "                    (b_q, slice(0, split_size)),\n",
    "                    (b_k, slice(split_size, 2 * split_size)),\n",
    "                    (b_v, slice(2 * split_size, 3 * split_size)),\n",
    "                ])\n",
    "        elif component.startswith(\"attn.c_proj\"):\n",
    "            if param_type == \"weight\":\n",
    "                new_name = f\"trf_blocks.{layer_idx}.attn.w_o.weight\"\n",
    "            else:\n",
    "                new_name = f\"trf_blocks.{layer_idx}.attn.w_o.bias\"\n",
    "            mappings.append((new_name, None))\n",
    "        elif component.startswith(\"mlp.c_fc\"):\n",
    "            if param_type == \"weight\":\n",
    "                new_name = f\"trf_blocks.{layer_idx}.ff.layers.0.weight\"\n",
    "            else:\n",
    "                new_name = f\"trf_blocks.{layer_idx}.ff.layers.0.bias\"\n",
    "            mappings.append((new_name, None))\n",
    "        elif component.startswith(\"mlp.c_proj\"):\n",
    "            if param_type == \"weight\":\n",
    "                new_name = f\"trf_blocks.{layer_idx}.ff.layers.2.weight\"\n",
    "            else:\n",
    "                new_name = f\"trf_blocks.{layer_idx}.ff.layers.2.bias\"\n",
    "            mappings.append((new_name, None))\n",
    "        return mappings\n",
    "\n",
    "    # If no mapping found\n",
    "    print(f\"Warning: No mapping found for parameter '{old_name}'\")\n",
    "    return mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_state_dict(initial_state_dict):\n",
    "    target_state_dict = {}\n",
    "    for old_name, tensor in initial_state_dict.items():\n",
    "        mapped = convert_name(old_name, tensor)\n",
    "        for new_name, slice_idx in mapped:\n",
    "            if slice_idx is not None:\n",
    "                # If the parameter needs to be split\n",
    "                new_tensor = tensor[(..., slice_idx)].clone()\n",
    "                target_state_dict[new_name] = new_tensor\n",
    "            else:\n",
    "                if \"ff\" in new_name and \"weight\" in new_name:\n",
    "                    target_state_dict[new_name] = tensor.clone().transpose(0, 1)\n",
    "                else:\n",
    "                    target_state_dict[new_name] = tensor.clone()\n",
    "    return target_state_dict\n",
    "\n",
    "converted_hf_model_state_dict = convert_state_dict(hf_model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.h.0.ln_1.weight torch.Size([768])\n",
      "transformer.h.0.ln_1.bias torch.Size([768])\n",
      "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.0.ln_2.weight torch.Size([768])\n",
      "transformer.h.0.ln_2.bias torch.Size([768])\n",
      "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "for k, v in hf_model_state_dict.items():\n",
    "    if \"transformer.h.0\" in k:\n",
    "        print(k, v.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trf_blocks.0.ln1.scale torch.Size([768])\n",
      "trf_blocks.0.ln1.shift torch.Size([768])\n",
      "trf_blocks.0.attn.w_q.weight torch.Size([768, 768])\n",
      "trf_blocks.0.attn.w_k.weight torch.Size([768, 768])\n",
      "trf_blocks.0.attn.w_v.weight torch.Size([768, 768])\n",
      "trf_blocks.0.attn.w_q.bias torch.Size([768])\n",
      "trf_blocks.0.attn.w_k.bias torch.Size([768])\n",
      "trf_blocks.0.attn.w_v.bias torch.Size([768])\n",
      "trf_blocks.0.attn.w_o.weight torch.Size([768, 768])\n",
      "trf_blocks.0.attn.w_o.bias torch.Size([768])\n",
      "trf_blocks.0.ln2.scale torch.Size([768])\n",
      "trf_blocks.0.ln2.shift torch.Size([768])\n",
      "trf_blocks.0.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.0.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.0.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.0.ff.layers.2.bias torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "for k, v in converted_hf_model_state_dict.items():\n",
    "    if \"trf_blocks.0\" in k:\n",
    "        print(k, v.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt2_small.trf_blocks[0].ff.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trf_blocks.0.ln1.scale torch.Size([768])\n",
      "trf_blocks.0.ln1.shift torch.Size([768])\n",
      "trf_blocks.0.attn.mask torch.Size([1024, 1024])\n",
      "trf_blocks.0.attn.w_q.weight torch.Size([768, 768])\n",
      "trf_blocks.0.attn.w_q.bias torch.Size([768])\n",
      "trf_blocks.0.attn.w_k.weight torch.Size([768, 768])\n",
      "trf_blocks.0.attn.w_k.bias torch.Size([768])\n",
      "trf_blocks.0.attn.w_v.weight torch.Size([768, 768])\n",
      "trf_blocks.0.attn.w_v.bias torch.Size([768])\n",
      "trf_blocks.0.attn.w_o.weight torch.Size([768, 768])\n",
      "trf_blocks.0.attn.w_o.bias torch.Size([768])\n",
      "trf_blocks.0.ln2.scale torch.Size([768])\n",
      "trf_blocks.0.ln2.shift torch.Size([768])\n",
      "trf_blocks.0.ff.layers.0.weight torch.Size([3072, 768])\n",
      "trf_blocks.0.ff.layers.0.bias torch.Size([3072])\n",
      "trf_blocks.0.ff.layers.2.weight torch.Size([768, 3072])\n",
      "trf_blocks.0.ff.layers.2.bias torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "for k, v in gpt2_small.state_dict().items():\n",
    "    if \"trf_blocks.0\" in k:\n",
    "        print(k, v.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [0., 0., 1.,  ..., 1., 1., 1.],\n",
       "        [0., 0., 0.,  ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 1., 1.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_gpt_state[\"trf_blocks.1.attn.mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_small.trf_blocks[1].attn.w_q.weight - gpt2_small_with_weights.trf_blocks[1].attn.w_q.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_weights_into_gpt(gpt, state_dict, backup_state_dict=None):\n",
    "    if backup_state_dict is not None:\n",
    "        final_state_dict = backup_state_dict\n",
    "    else:\n",
    "        final_state_dict = {}\n",
    "    final_state_dict.update(state_dict)\n",
    "    gpt.load_state_dict(final_state_dict)\n",
    "    return gpt\n",
    "\n",
    "gpt2_small_with_weights = load_weights_into_gpt(\n",
    "    gpt2_small, \n",
    "    converted_hf_model_state_dict, \n",
    "    gpt2_small.state_dict()\n",
    ")\n",
    "gpt2_small_with_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1632, -11.0749, -14.1920,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         [ -9.8438,  -9.7723, -12.6191,  ..., -16.6923, -15.9322,  -9.6007]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1632, -11.0749, -14.1920,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         [ -9.8438,  -9.7723, -12.6191,  ..., -16.6923, -15.9322,  -9.6007],\n",
      "         [ -9.8205,  -9.6539, -12.5554,  ..., -16.4342, -15.9177,  -9.6153]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1632, -11.0749, -14.1920,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         [ -9.8438,  -9.7723, -12.6191,  ..., -16.6923, -15.9322,  -9.6007],\n",
      "         [ -9.8205,  -9.6539, -12.5554,  ..., -16.4342, -15.9177,  -9.6153],\n",
      "         [-10.3168, -10.2691, -13.1820,  ..., -16.9963, -16.5753, -10.0659]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1632, -11.0749, -14.1920,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [ -9.8205,  -9.6539, -12.5554,  ..., -16.4342, -15.9177,  -9.6153],\n",
      "         [-10.3168, -10.2691, -13.1820,  ..., -16.9963, -16.5753, -10.0659],\n",
      "         [ -9.5892,  -9.4860, -12.2889,  ..., -15.9200, -15.6799,  -9.3871]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1631, -11.0749, -14.1920,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [-10.3168, -10.2691, -13.1820,  ..., -16.9963, -16.5753, -10.0659],\n",
      "         [ -9.5892,  -9.4860, -12.2889,  ..., -15.9200, -15.6799,  -9.3871],\n",
      "         [-10.3921, -10.1924, -13.0855,  ..., -17.2299, -16.7621, -10.1666]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1632, -11.0749, -14.1920,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [ -9.5892,  -9.4860, -12.2889,  ..., -15.9200, -15.6799,  -9.3871],\n",
      "         [-10.3921, -10.1924, -13.0855,  ..., -17.2299, -16.7621, -10.1666],\n",
      "         [-10.4727, -10.4776, -13.3645,  ..., -17.1444, -16.6672, -10.2782]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1632, -11.0749, -14.1920,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [-10.3921, -10.1924, -13.0855,  ..., -17.2299, -16.7621, -10.1666],\n",
      "         [-10.4727, -10.4776, -13.3645,  ..., -17.1444, -16.6672, -10.2782],\n",
      "         [ -9.9671,  -9.8887, -12.7450,  ..., -16.3500, -15.8732,  -9.7360]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1632, -11.0749, -14.1920,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [-10.4727, -10.4776, -13.3645,  ..., -17.1444, -16.6672, -10.2782],\n",
      "         [ -9.9671,  -9.8887, -12.7450,  ..., -16.3500, -15.8732,  -9.7360],\n",
      "         [-10.6131, -10.4056, -13.2964,  ..., -17.3649, -16.7824, -10.3264]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1632, -11.0749, -14.1920,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [ -9.9671,  -9.8887, -12.7450,  ..., -16.3500, -15.8732,  -9.7360],\n",
      "         [-10.6131, -10.4056, -13.2964,  ..., -17.3649, -16.7824, -10.3264],\n",
      "         [-10.4316, -10.3914, -13.2257,  ..., -16.7527, -16.2950, -10.2160]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1632, -11.0749, -14.1920,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [-10.6131, -10.4056, -13.2964,  ..., -17.3649, -16.7824, -10.3264],\n",
      "         [-10.4316, -10.3914, -13.2257,  ..., -16.7527, -16.2950, -10.2160],\n",
      "         [-10.7795, -10.5474, -13.4478,  ..., -17.4982, -16.8747, -10.4380]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1632, -11.0749, -14.1920,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [-10.4316, -10.3914, -13.2257,  ..., -16.7527, -16.2950, -10.2160],\n",
      "         [-10.7795, -10.5474, -13.4478,  ..., -17.4982, -16.8747, -10.4380],\n",
      "         [-10.6918, -10.6868, -13.5572,  ..., -17.2353, -16.6094, -10.4238]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1632, -11.0749, -14.1920,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [-10.7795, -10.5474, -13.4478,  ..., -17.4982, -16.8747, -10.4380],\n",
      "         [-10.6918, -10.6868, -13.5572,  ..., -17.2353, -16.6094, -10.4238],\n",
      "         [-10.6610, -10.6442, -13.4962,  ..., -17.1572, -16.5312, -10.3717]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1631, -11.0749, -14.1920,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [-10.6918, -10.6868, -13.5572,  ..., -17.2353, -16.6094, -10.4238],\n",
      "         [-10.6610, -10.6442, -13.4962,  ..., -17.1572, -16.5312, -10.3717],\n",
      "         [ -9.8541,  -9.8324, -12.6726,  ..., -15.8825, -15.5331,  -9.6777]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1631, -11.0749, -14.1920,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [-10.6610, -10.6442, -13.4962,  ..., -17.1572, -16.5312, -10.3717],\n",
      "         [ -9.8541,  -9.8324, -12.6726,  ..., -15.8825, -15.5331,  -9.6777],\n",
      "         [ -9.9058,  -9.8577, -12.6825,  ..., -15.9571, -15.6451,  -9.7254]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1631, -11.0749, -14.1920,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [ -9.8541,  -9.8324, -12.6726,  ..., -15.8825, -15.5331,  -9.6777],\n",
      "         [ -9.9058,  -9.8577, -12.6825,  ..., -15.9571, -15.6451,  -9.7254],\n",
      "         [-10.4711, -10.3604, -13.1737,  ..., -16.6664, -16.2516, -10.2232]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1631, -11.0749, -14.1920,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [ -9.9058,  -9.8577, -12.6825,  ..., -15.9571, -15.6451,  -9.7254],\n",
      "         [-10.4711, -10.3604, -13.1737,  ..., -16.6664, -16.2516, -10.2232],\n",
      "         [-10.7837, -10.4945, -13.3476,  ..., -17.3391, -16.7834, -10.4198]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1631, -11.0749, -14.1920,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [-10.4711, -10.3604, -13.1737,  ..., -16.6664, -16.2516, -10.2232],\n",
      "         [-10.7837, -10.4945, -13.3476,  ..., -17.3391, -16.7834, -10.4198],\n",
      "         [-10.4807, -10.3440, -13.1603,  ..., -16.6017, -16.1792, -10.2121]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1631, -11.0749, -14.1920,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [-10.7837, -10.4945, -13.3476,  ..., -17.3391, -16.7834, -10.4198],\n",
      "         [-10.4807, -10.3440, -13.1603,  ..., -16.6017, -16.1792, -10.2121],\n",
      "         [-10.4479, -10.3000, -13.1078,  ..., -16.5068, -16.1121, -10.1716]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1631, -11.0749, -14.1920,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [-10.4807, -10.3440, -13.1603,  ..., -16.6017, -16.1792, -10.2121],\n",
      "         [-10.4479, -10.3000, -13.1078,  ..., -16.5068, -16.1121, -10.1716],\n",
      "         [-10.0940,  -9.9396, -12.7515,  ..., -16.1568, -15.6605,  -9.7835]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1631, -11.0749, -14.1920,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [-10.4479, -10.3000, -13.1078,  ..., -16.5068, -16.1121, -10.1716],\n",
      "         [-10.0940,  -9.9396, -12.7515,  ..., -16.1568, -15.6605,  -9.7835],\n",
      "         [-10.7488, -10.4321, -13.2802,  ..., -17.1885, -16.6297, -10.3613]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1631, -11.0749, -14.1920,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [-10.0940,  -9.9396, -12.7515,  ..., -16.1568, -15.6605,  -9.7835],\n",
      "         [-10.7488, -10.4321, -13.2802,  ..., -17.1885, -16.6297, -10.3613],\n",
      "         [-10.7481, -10.4057, -13.2785,  ..., -17.1803, -16.5968, -10.3459]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1631, -11.0749, -14.1920,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [-10.7488, -10.4321, -13.2802,  ..., -17.1885, -16.6297, -10.3613],\n",
      "         [-10.7481, -10.4057, -13.2785,  ..., -17.1803, -16.5968, -10.3459],\n",
      "         [-10.7541, -10.3840, -13.2804,  ..., -17.1720, -16.5604, -10.3310]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1631, -11.0749, -14.1920,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [-10.7481, -10.4057, -13.2785,  ..., -17.1803, -16.5968, -10.3459],\n",
      "         [-10.7541, -10.3840, -13.2804,  ..., -17.1720, -16.5604, -10.3310],\n",
      "         [ -9.7251,  -9.6362, -12.4467,  ..., -15.5488, -15.1603,  -9.5240]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1631, -11.0749, -14.1920,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [-10.7541, -10.3840, -13.2804,  ..., -17.1720, -16.5604, -10.3310],\n",
      "         [ -9.7251,  -9.6362, -12.4467,  ..., -15.5488, -15.1603,  -9.5240],\n",
      "         [-10.7660, -10.3618, -13.2600,  ..., -17.1660, -16.5482, -10.3307]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1631, -11.0749, -14.1920,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [ -9.7251,  -9.6362, -12.4467,  ..., -15.5488, -15.1603,  -9.5240],\n",
      "         [-10.7660, -10.3618, -13.2600,  ..., -17.1660, -16.5482, -10.3307],\n",
      "         [ -9.7708,  -9.6489, -12.4611,  ..., -15.5891, -15.1976,  -9.5621]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1631, -11.0749, -14.1920,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [-10.7660, -10.3618, -13.2600,  ..., -17.1660, -16.5482, -10.3307],\n",
      "         [ -9.7708,  -9.6489, -12.4611,  ..., -15.5891, -15.1976,  -9.5621],\n",
      "         [-10.4405, -10.2298, -13.0231,  ..., -16.4084, -15.9650, -10.0984]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1631, -11.0749, -14.1920,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [ -9.7708,  -9.6489, -12.4611,  ..., -15.5891, -15.1976,  -9.5621],\n",
      "         [-10.4405, -10.2298, -13.0231,  ..., -16.4084, -15.9650, -10.0984],\n",
      "         [ -9.8404,  -9.6953, -12.4824,  ..., -15.6105, -15.2550,  -9.6209]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1631, -11.0749, -14.1920,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [-10.4405, -10.2298, -13.0231,  ..., -16.4084, -15.9650, -10.0984],\n",
      "         [ -9.8404,  -9.6953, -12.4824,  ..., -15.6105, -15.2550,  -9.6209],\n",
      "         [-10.8127, -10.3604, -13.2404,  ..., -17.1725, -16.5691, -10.3595]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1631, -11.0749, -14.1920,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [ -9.8404,  -9.6953, -12.4824,  ..., -15.6105, -15.2550,  -9.6209],\n",
      "         [-10.8127, -10.3604, -13.2404,  ..., -17.1725, -16.5691, -10.3595],\n",
      "         [-10.6486, -10.4940, -13.2878,  ..., -16.8300, -16.2683, -10.2641]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1632, -11.0749, -14.1921,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [-10.8127, -10.3604, -13.2404,  ..., -17.1725, -16.5691, -10.3595],\n",
      "         [-10.6486, -10.4940, -13.2878,  ..., -16.8300, -16.2683, -10.2641],\n",
      "         [-10.6267, -10.4628, -13.2467,  ..., -16.7749, -16.2153, -10.2265]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1632, -11.0749, -14.1921,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [-10.6486, -10.4940, -13.2878,  ..., -16.8300, -16.2683, -10.2641],\n",
      "         [-10.6267, -10.4628, -13.2467,  ..., -16.7749, -16.2153, -10.2265],\n",
      "         [-10.3960, -10.1424, -12.9087,  ..., -16.2611, -15.8358, -10.0307]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1632, -11.0749, -14.1921,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [-10.6267, -10.4628, -13.2467,  ..., -16.7749, -16.2153, -10.2265],\n",
      "         [-10.3960, -10.1424, -12.9087,  ..., -16.2611, -15.8358, -10.0307],\n",
      "         [-10.5624, -10.3842, -13.1520,  ..., -16.6452, -16.1090, -10.1472]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1632, -11.0749, -14.1921,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [-10.3960, -10.1424, -12.9087,  ..., -16.2611, -15.8358, -10.0307],\n",
      "         [-10.5624, -10.3842, -13.1520,  ..., -16.6452, -16.1090, -10.1472],\n",
      "         [-10.3719, -10.1059, -12.8529,  ..., -16.1933, -15.7870,  -9.9919]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1632, -11.0749, -14.1921,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [-10.5624, -10.3842, -13.1520,  ..., -16.6452, -16.1090, -10.1472],\n",
      "         [-10.3719, -10.1059, -12.8529,  ..., -16.1933, -15.7870,  -9.9919],\n",
      "         [-10.3648, -10.0926, -12.8342,  ..., -16.1676, -15.7799,  -9.9779]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1632, -11.0749, -14.1921,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [-10.3719, -10.1059, -12.8529,  ..., -16.1933, -15.7870,  -9.9919],\n",
      "         [-10.3648, -10.0926, -12.8342,  ..., -16.1676, -15.7799,  -9.9779],\n",
      "         [ -9.8625,  -9.6683, -12.4803,  ..., -15.5558, -15.0426,  -9.6251]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1632, -11.0749, -14.1921,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [-10.3648, -10.0926, -12.8342,  ..., -16.1676, -15.7799,  -9.9779],\n",
      "         [ -9.8625,  -9.6683, -12.4803,  ..., -15.5558, -15.0426,  -9.6251],\n",
      "         [-10.5463, -10.3489, -13.1013,  ..., -16.5925, -16.0950, -10.1350]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1632, -11.0749, -14.1921,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [ -9.8625,  -9.6683, -12.4803,  ..., -15.5558, -15.0426,  -9.6251],\n",
      "         [-10.5463, -10.3489, -13.1013,  ..., -16.5925, -16.0950, -10.1350],\n",
      "         [ -9.9038,  -9.6951, -12.5102,  ..., -15.6057, -15.0809,  -9.6909]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1632, -11.0749, -14.1921,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [-10.5463, -10.3489, -13.1013,  ..., -16.5925, -16.0950, -10.1350],\n",
      "         [ -9.9038,  -9.6951, -12.5102,  ..., -15.6057, -15.0809,  -9.6909],\n",
      "         [-10.8674, -10.3492, -13.2114,  ..., -17.0915, -16.4964, -10.4366]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1632, -11.0749, -14.1921,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [ -9.9038,  -9.6951, -12.5102,  ..., -15.6057, -15.0809,  -9.6909],\n",
      "         [-10.8674, -10.3492, -13.2114,  ..., -17.0915, -16.4964, -10.4366],\n",
      "         [-10.5315, -10.3228, -13.0808,  ..., -16.5758, -16.0560, -10.1294]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1632, -11.0749, -14.1921,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [-10.8674, -10.3492, -13.2114,  ..., -17.0915, -16.4964, -10.4366],\n",
      "         [-10.5315, -10.3228, -13.0808,  ..., -16.5758, -16.0560, -10.1294],\n",
      "         [ -9.9328,  -9.7320, -12.4875,  ..., -15.5572, -15.1788,  -9.7430]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1632, -11.0749, -14.1921,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [-10.5315, -10.3228, -13.0808,  ..., -16.5758, -16.0560, -10.1294],\n",
      "         [ -9.9328,  -9.7320, -12.4875,  ..., -15.5572, -15.1788,  -9.7430],\n",
      "         [-10.0240,  -9.8081, -12.5530,  ..., -15.6565, -15.2992,  -9.8373]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1632, -11.0749, -14.1921,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [ -9.9328,  -9.7320, -12.4875,  ..., -15.5572, -15.1788,  -9.7430],\n",
      "         [-10.0240,  -9.8081, -12.5530,  ..., -15.6565, -15.2992,  -9.8373],\n",
      "         [-10.0782,  -9.8434, -12.5623,  ..., -15.6941, -15.3597,  -9.8911]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1632, -11.0749, -14.1921,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [-10.0240,  -9.8081, -12.5530,  ..., -15.6565, -15.2992,  -9.8373],\n",
      "         [-10.0782,  -9.8434, -12.5623,  ..., -15.6941, -15.3597,  -9.8911],\n",
      "         [-10.6413, -10.3960, -13.1259,  ..., -16.6743, -16.1778, -10.2296]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1632, -11.0749, -14.1921,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [-10.0782,  -9.8434, -12.5623,  ..., -15.6941, -15.3597,  -9.8911],\n",
      "         [-10.6413, -10.3960, -13.1259,  ..., -16.6743, -16.1778, -10.2296],\n",
      "         [-10.9522, -10.3705, -13.2036,  ..., -17.1412, -16.5502, -10.5163]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1632, -11.0749, -14.1921,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [-10.6413, -10.3960, -13.1259,  ..., -16.6743, -16.1778, -10.2296],\n",
      "         [-10.9522, -10.3705, -13.2036,  ..., -17.1412, -16.5502, -10.5163],\n",
      "         [-10.4822, -10.1543, -12.8682,  ..., -16.2797, -15.8911, -10.1065]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1632, -11.0749, -14.1921,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [-10.9522, -10.3705, -13.2036,  ..., -17.1412, -16.5502, -10.5163],\n",
      "         [-10.4822, -10.1543, -12.8682,  ..., -16.2797, -15.8911, -10.1065],\n",
      "         [-10.9732, -10.3746, -13.2128,  ..., -17.1597, -16.5512, -10.5264]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1632, -11.0749, -14.1921,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [-10.4822, -10.1543, -12.8682,  ..., -16.2797, -15.8911, -10.1065],\n",
      "         [-10.9732, -10.3746, -13.2128,  ..., -17.1597, -16.5512, -10.5264],\n",
      "         [-10.6263, -10.3663, -13.0966,  ..., -16.6322, -16.1039, -10.2121]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1632, -11.0749, -14.1921,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [-10.9732, -10.3746, -13.2128,  ..., -17.1597, -16.5512, -10.5264],\n",
      "         [-10.6263, -10.3663, -13.0966,  ..., -16.6322, -16.1039, -10.2121],\n",
      "         [-10.1612,  -9.8976, -12.6041,  ..., -15.7509, -15.3770,  -9.9616]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1632, -11.0749, -14.1921,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [-10.6263, -10.3663, -13.0966,  ..., -16.6322, -16.1039, -10.2121],\n",
      "         [-10.1612,  -9.8976, -12.6041,  ..., -15.7509, -15.3770,  -9.9616],\n",
      "         [-10.6375, -10.3686, -13.0853,  ..., -16.6320, -16.1074, -10.2188]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ -8.1808,  -8.0697, -10.9996,  ..., -14.9142, -14.5079,  -8.0091],\n",
      "         [-11.4183, -11.3589, -14.6981,  ..., -18.7968, -18.2219, -11.8120],\n",
      "         [-11.1632, -11.0749, -14.1921,  ..., -18.0516, -17.8552, -11.1309],\n",
      "         ...,\n",
      "         [-10.1612,  -9.8976, -12.6041,  ..., -15.7509, -15.3770,  -9.9616],\n",
      "         [-10.6375, -10.3686, -13.0853,  ..., -16.6320, -16.1074, -10.2188],\n",
      "         [-11.0305, -10.3947, -13.2233,  ..., -17.1965, -16.5700, -10.5770]]],\n",
      "       device='cuda:0')\n",
      "Every effort moves you and. the . and , .. the the, ,, and    the  the, the ..,.,, a. a . the the the. , . the. ,\n"
     ]
    }
   ],
   "source": [
    "generate_and_print_with_decoding(gpt2_small_with_weights, tokenizer, torch_device, \"Every effort moves you\", topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy(model, token_ids, max_new_tokens, context_size):\n",
    "    logits = None\n",
    "    for i in range(max_new_tokens):\n",
    "        context_token_ids = token_ids[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(context_token_ids)\n",
    "\n",
    "        logits = logits[:, -1, :]\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        token_id_next = torch.argmax(probas, dim=1, keepdim=True)  # Pure Greed\n",
    "        token_ids = torch.cat((token_ids, token_id_next), dim=1) \n",
    "\n",
    "    return token_ids\n",
    "\n",
    "def topk(model, token_ids, max_new_tokens, context_size, temperature=1.0, k=5):\n",
    "    logits = None\n",
    "    if temperature == 0:\n",
    "        return greedy(model, token_ids, max_new_tokens, context_size)\n",
    "    for i in range(max_new_tokens):\n",
    "        context_token_ids = token_ids[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(context_token_ids)\n",
    "        if not isinstance(logits, torch.Tensor):\n",
    "            logits = logits[0]\n",
    "        logits = logits[:, -1, :]\n",
    "        scaled_logits = logits/temperature\n",
    "        _, inds = torch.topk(scaled_logits, dim=-1, k=k)\n",
    "        mask = torch.ones_like(scaled_logits, dtype=torch.bool).scatter_(-1, inds, False)\n",
    "        topk_scaled_logits = scaled_logits.masked_fill_(mask, -torch.inf)\n",
    "        probas = torch.softmax(topk_scaled_logits, dim=-1)\n",
    "        token_id_next = torch.multinomial(probas, num_samples=1)  # Pure Random\n",
    "        token_ids = torch.cat((token_ids, token_id_next), dim=1) \n",
    "\n",
    "    return token_ids\n",
    "\n",
    "def generate_and_print_with_decoding(model, tokenizer, device, start_context, decoding_fn, context_size=None):\n",
    "    model.eval()\n",
    "    if context_size is None:\n",
    "        context_size = model.pos_emb.weight.shape[0]\n",
    "    token_ids = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = decoding_fn(model, token_ids, 50, context_size)\n",
    "    decoded_text = tokenizer.decode(token_ids[0].tolist())\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you to a higher level. It is your responsibility to make sure you are not only successful in the world, but also the most important. You must make the best decisions for your life.  You must make the best decisions for your life. The\n"
     ]
    }
   ],
   "source": [
    "generate_and_print_with_decoding(\n",
    "    hf_model.to(torch_device), \n",
    "    tokenizer, \n",
    "    torch_device, \n",
    "    \"Every effort moves you\", \n",
    "    topk, \n",
    "    context_size=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jejllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    torch_device = torch.device(\"mps:0\")\n",
    "    x = torch.ones(1, device=torch_device)\n",
    "    print (x)\n",
    "else:\n",
    "    torch_device = torch.device(\"cpu\")\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "EMBEDDING_DIM = 3\n",
    "CONTEXT_LENGTH = 4\n",
    "QKV_DIM = 2\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length=CONTEXT_LENGTH, dropout=0.5, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.w_q = nn.Linear(d_in, d_out, bias=qkv_bias).to(torch_device)\n",
    "        self.w_k = nn.Linear(d_in, d_out, bias=qkv_bias).to(torch_device)\n",
    "        self.w_v = nn.Linear(d_in, d_out, bias=qkv_bias).to(torch_device)\n",
    "        self.dropout = nn.Dropout(dropout).to(torch_device)\n",
    "        self.register_buffer(\n",
    "            'mask', \n",
    "            torch.triu(\n",
    "                torch.ones(context_length, context_length), \n",
    "                diagonal=1,\n",
    "            ).to(torch_device)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        num_tokens = x.shape[-2]\n",
    "        queries = self.w_q(x)\n",
    "        keys = self.w_k(x)\n",
    "        attn_scores = queries @ keys.transpose(-2, -1)\n",
    "        causal_attn_scores = attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        causal_attn_weights = torch.softmax(causal_attn_scores*(self.d_out**0.5), dim=-1)\n",
    "        causal_attn_weights = self.dropout(causal_attn_weights)\n",
    "        values = self.w_v(x)\n",
    "        context = causal_attn_weights @ values\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "EMBEDDING_DIM = 3\n",
    "CONTEXT_LENGTH = 4\n",
    "QKV_DIM = 2\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out//num_heads\n",
    "        self.w_q = nn.Linear(d_in, d_out, bias=qkv_bias).to(torch_device)\n",
    "        self.w_k = nn.Linear(d_in, d_out, bias=qkv_bias).to(torch_device)\n",
    "        self.w_v = nn.Linear(d_in, d_out, bias=qkv_bias).to(torch_device)\n",
    "        self.w_o = nn.Linear(d_out, d_out).to(torch_device)\n",
    "        # self.w_o = nn.Identity().to(torch_device)\n",
    "        self.dropout = nn.Dropout(dropout).to(torch_device)\n",
    "        self.register_buffer(\n",
    "            'mask', \n",
    "            torch.triu(\n",
    "                torch.ones(context_length, context_length), \n",
    "                diagonal=1,\n",
    "            ).to(torch_device)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.w_k(x)\n",
    "        queries = self.w_q(x)\n",
    "        values = self.w_v(x)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(-3, -2)\n",
    "        # TODO: KV Cache Optimization\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim).transpose(-3, -2)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim).transpose(-3, -2)\n",
    "        attn_scores = queries @ keys.transpose(-2, -1)\n",
    "        causal_attn_scores = attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        causal_attn_weights = torch.softmax(causal_attn_scores/(keys.shape[-1]**0.5), dim=-1)\n",
    "        causal_attn_weights = self.dropout(causal_attn_weights)\n",
    "        context = (causal_attn_weights @ values).transpose(-3, -2)\n",
    "        context = context.contiguous().view(b, num_tokens, self.num_heads*self.head_dim)\n",
    "        context = self.w_o(context)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(d_model))\n",
    "        self.shift = nn.Parameter(torch.zeros(d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True, unbiased=False)\n",
    "        x = (self.scale * (x - mean) / (std + self.eps)) + self.shift\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (\n",
    "            1 + torch.tanh(\n",
    "                torch.sqrt(torch.tensor(2.0/torch.pi, device=torch_device)) * (x  + 0.044715 * x**3)\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward (nn.Module):\n",
    "    def __init__(self, cfg): \n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], cfg[\"emb_dim\"]*4).to(torch_device),\n",
    "            GELU().to(torch_device),\n",
    "            nn.Linear(cfg[\"emb_dim\"]*4, cfg[\"emb_dim\"]).to(torch_device),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = LayerNorm(config[\"emb_dim\"]).to(torch_device)\n",
    "        self.attn = MultiHeadAttention(config[\"emb_dim\"], config[\"emb_dim\"], config[\"context_length\"], config[\"drop_rate\"], config[\"n_heads\"], config[\"qkv_bias\"]).to(torch_device)\n",
    "        self.drop = nn.Dropout(config[\"drop_rate\"]).to(torch_device)\n",
    "        self.ln2 = LayerNorm(config[\"emb_dim\"]).to(torch_device)\n",
    "        self.ff = FeedForward(config).to(torch_device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.drop(self.attn(self.ln1(x))) + x\n",
    "        x = self.drop(self.ff(self.ln2(x))) + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"]).to(torch_device)\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"]).to(torch_device)\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"]).to(torch_device)\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        ).to(torch_device)\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"]).to(torch_device)\n",
    "        # GPT2 uses tied weights for the embedding and output layers\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False).to(torch_device)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        in_idx = in_idx.to(torch_device)\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=torch_device))\n",
    "        x = self.drop_emb(tok_embeds + pos_embeds)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, token_ids, max_new_tokens, context_size):\n",
    "    logits = None\n",
    "    for i in range(max_new_tokens):\n",
    "        context_token_ids = token_ids[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(context_token_ids)\n",
    "\n",
    "        logits = logits[:, -1, :]\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        token_id_next = torch.argmax(probas, dim=1, keepdim=True)  # Pure Greed\n",
    "        token_ids = torch.cat((token_ids, token_id_next), dim=1) \n",
    "\n",
    "    return token_ids\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special=set(['<|endoftext|>']))\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0).to(torch_device)\n",
    "    return encoded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logits, targets):\n",
    "    vocab_size = logits.shape[-1]\n",
    "    loss = nn.CrossEntropyLoss()(logits.view(-1, vocab_size), targets.view(-1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "#     def __init__(self, txt, tokenizer, max_length, stride):\n",
    "#         self.input_ids = []\n",
    "#         self.target_ids = []\n",
    "#         token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "#         token_ids = token_ids\n",
    "#         token_ids = torch.tensor(token_ids).to(torch_device)\n",
    "#         print(token_ids.shape)\n",
    "#         token_sequences = token_ids.unfold(0, max_length, stride)\n",
    "#         self.input_ids = token_sequences[:-1]\n",
    "#         self.target_ids = token_sequences[1:]\n",
    "\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "def create_dataloader_v1(\n",
    "        txt, batch_size=4, max_length=256, \n",
    "        stride=128, shuffle=True, drop_last=True,\n",
    "        num_workers=0,\n",
    "    ):\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    return DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device=torch_device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.view(-1, logits.shape[-1]), \n",
    "        target_batch.view(-1),\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(dataloader, model, device, num_batches=None):\n",
    "    total_loss = 0\n",
    "    if len(dataloader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(dataloader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(dataloader))\n",
    "    for i, (input_batch, target_batch) in enumerate(dataloader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_dataloader, test_dataloader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_dataloader, model, device, num_batches=eval_iter)\n",
    "        test_loss = calc_loss_loader(test_dataloader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, test_loss\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    token_ids = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(model, token_ids, 50, context_size)\n",
    "    decoded_text = tokenizer.decode(token_ids[0].tolist())\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(\n",
    "    model, train_dataloader, test_dataloader,\n",
    "    optimizer, device, num_epochs,\n",
    "    eval_freq, eval_iter, start_context, tokenizer\n",
    "):\n",
    "    train_losses, test_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            # print(input_batch)\n",
    "            # print(target_batch)\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            loss.backward()\n",
    "            # grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            # print(f\"Gradient norm: {grad_norm:.4f}\")\n",
    "            optimizer.step()\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step %eval_freq == 0:\n",
    "                train_loss, test_loss = evaluate_model(\n",
    "                    model, train_dataloader, test_dataloader, device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                test_losses.append(test_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(\n",
    "                    f\"Epoch {epoch + 1} (Step {global_step:06d}): \"\n",
    "                    f\"Train Loss: {train_loss:.3f}, \"\n",
    "                    f\"Val Loss: {test_loss:.3f}\"\n",
    "                )\n",
    "\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "    return train_losses, test_losses, track_tokens_seen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "train_ratio = 0.9\n",
    "split_idx = int(len(raw_text) * train_ratio)\n",
    "train_data = raw_text[:split_idx]\n",
    "test_data = raw_text[split_idx:]\n",
    "\n",
    "torch.manual_seed(123)\n",
    "train_dataloader = create_dataloader_v1(train_data, batch_size=2, max_length=GPT_CONFIG_124M[\"context_length\"], stride=GPT_CONFIG_124M[\"context_length\"], shuffle=True)\n",
    "test_dataloader = create_dataloader_v1(test_data, batch_size=2, max_length=GPT_CONFIG_124M[\"context_length\"], stride=GPT_CONFIG_124M[\"context_length\"], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data[:100], test_data[:100]\n",
    "# tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "# tokenizer.encode(train_data[:100], allowed_special={\"<|endoftext|>\"})\n",
    "# len(train_dataloader), len(test_dataloader)\n",
    "# input_sample, target_sample = next(iter(train_dataloader))\n",
    "# torch.sum(input_sample[1]), torch.sum(target_sample[1])\n",
    "# input_sample.shape, \n",
    "# input_sample[1],target_sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "gpt = GPTModel(GPT_CONFIG_124M).to(torch_device)\n",
    "# gpt = torch.compile(gpt).to(torch_device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    gpt.parameters(),\n",
    "    lr = 0.0004, weight_decay = 0.1\n",
    ")\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Step 000000): Train Loss: 9.817, Val Loss: 9.924\n",
      "Epoch 1 (Step 000005): Train Loss: 8.066, Val Loss: 8.332\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Epoch 2 (Step 000010): Train Loss: 6.619, Val Loss: 7.042\n",
      "Epoch 2 (Step 000015): Train Loss: 6.046, Val Loss: 6.596\n",
      "Every effort moves you, and,, and, and,,,,, and, and,,,,,,,,,,, and,, the,, the, and,, and,,, the, and,,,,,,\n",
      "Epoch 3 (Step 000020): Train Loss: 5.524, Val Loss: 6.508\n",
      "Epoch 3 (Step 000025): Train Loss: 5.369, Val Loss: 6.378\n",
      "Every effort moves you, and to the of the of the picture. Gis.                                     \n",
      "Epoch 4 (Step 000030): Train Loss: 4.830, Val Loss: 6.263\n",
      "Epoch 4 (Step 000035): Train Loss: 4.586, Val Loss: 6.285\n",
      "Every effort moves you of the \"I the picture.                    \"I\"I the picture\"I had the picture\"I the picture and I had been the picture of\n",
      "Epoch 5 (Step 000040): Train Loss: 3.880, Val Loss: 6.130\n",
      "Every effort moves you know he had been his pictures, and I felt it's by his last word.                   \"Oh, and he had been the end, and he had been\n",
      "Epoch 6 (Step 000045): Train Loss: 3.530, Val Loss: 6.183\n",
      "Epoch 6 (Step 000050): Train Loss: 2.960, Val Loss: 6.123\n",
      "Every effort moves you know it was his pictures--I glanced after him, I had the last word.        \"Oh, and I was his pictures--I looked.   \"I looked. \"I looked. \n",
      "Epoch 7 (Step 000055): Train Loss: 2.832, Val Loss: 6.150\n",
      "Epoch 7 (Step 000060): Train Loss: 2.104, Val Loss: 6.133\n",
      "Every effort moves you know the picture to me--I glanced after him, and Mrs.  \"I was no great, the fact, the fact that, the moment--as Jack himself, as his pictures--as of the picture--because he was a little\n",
      "Epoch 8 (Step 000065): Train Loss: 1.691, Val Loss: 6.186\n",
      "Epoch 8 (Step 000070): Train Loss: 1.391, Val Loss: 6.230\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the fact with a little: \"Yes--and by me to me to have to see a smile behind his close grayish beard--as if he had the donkey. \"There were days when I\n",
      "Epoch 9 (Step 000075): Train Loss: 1.059, Val Loss: 6.251\n",
      "Epoch 9 (Step 000080): Train Loss: 0.800, Val Loss: 6.278\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the fact with a laugh: \"Yes--and by me!\"  He laughed again, and threw back the window-curtains, I saw that, and down the room, and now\n",
      "Epoch 10 (Step 000085): Train Loss: 0.569, Val Loss: 6.373\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n"
     ]
    }
   ],
   "source": [
    "train_losses, test_losses, track_tokens_seen = train_model_simple(\n",
    "    gpt, train_dataloader, test_dataloader, optimizer, torch_device,\n",
    "    num_epochs, eval_freq=5, eval_iter=5, start_context=\"Every effort moves you\", tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, test_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, test_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, track_tokens_seen, train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy(model, token_ids, max_new_tokens, context_size):\n",
    "    logits = None\n",
    "    for i in range(max_new_tokens):\n",
    "        context_token_ids = token_ids[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(context_token_ids)\n",
    "\n",
    "        logits = logits[:, -1, :]\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        token_id_next = torch.argmax(probas, dim=1, keepdim=True)  # Pure Greed\n",
    "        token_ids = torch.cat((token_ids, token_id_next), dim=1) \n",
    "\n",
    "    return token_ids\n",
    "\n",
    "def generate_and_print_with_decoding(model, tokenizer, device, start_context, decoding_fn):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    token_ids = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = decoding_fn(model, token_ids, 50, context_size)\n",
    "    decoded_text = tokenizer.decode(token_ids[0].tolist())\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n"
     ]
    }
   ],
   "source": [
    "generate_and_print_with_decoding(gpt, tokenizer, torch_device, \"Every effort moves you\", greedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_tokens(model, token_ids, max_new_tokens, context_size, top_k=5):\n",
    "    logits = None\n",
    "    for i in range(max_new_tokens):\n",
    "        context_token_ids = token_ids[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(context_token_ids)\n",
    "\n",
    "        logits = logits[:, -1, :]\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        token_id_next = torch.multinomial(probas, num_samples=1)  # Pure Random\n",
    "        token_ids = torch.cat((token_ids, token_id_next), dim=1) \n",
    "        # Print the top-k tokens\n",
    "        topk_probs, topk_tokens = torch.topk(probas, top_k, dim=-1)\n",
    "        for topk_prob, topk_token in zip(topk_probs[0], topk_tokens[0]):\n",
    "            token_str = tokenizer.decode([topk_token])\n",
    "            print(f\"{token_str}: {topk_prob:.4f}\")\n",
    "    \n",
    "    return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random(model, token_ids, max_new_tokens, context_size, temperature=1.0):\n",
    "    logits = None\n",
    "    if temperature == 0:\n",
    "        return greedy(model, token_ids, max_new_tokens, context_size)\n",
    "    for i in range(max_new_tokens):\n",
    "        context_token_ids = token_ids[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(context_token_ids)\n",
    "\n",
    "        logits = logits[:, -1, :]\n",
    "        probas = torch.softmax(logits/temperature, dim=-1)\n",
    "        token_id_next = torch.multinomial(probas, num_samples=1)  # Pure Random\n",
    "        token_ids = torch.cat((token_ids, token_id_next), dim=1) \n",
    "\n",
    "    return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\": 0.2708\n",
      " know: 0.1364\n",
      " in: 0.0385\n",
      " say: 0.0149\n",
      " can: 0.0144\n",
      ",\": 0.0813\n",
      ";: 0.0580\n",
      ".\": 0.0332\n",
      " terr: 0.0255\n",
      " began: 0.0222\n",
      " know: 0.1032\n",
      " say: 0.0476\n",
      " like: 0.0341\n",
      "?\": 0.0165\n",
      " G: 0.0148\n",
      " to: 0.9591\n",
      " go: 0.0017\n",
      " one: 0.0014\n",
      " on: 0.0012\n",
      " not: 0.0007\n",
      " do: 0.2951\n",
      " see: 0.0329\n",
      " go: 0.0289\n",
      " tears: 0.0231\n",
      " put: 0.0180\n",
      " the: 0.9272\n",
      " his: 0.0016\n",
      " spite: 0.0016\n",
      " my: 0.0016\n",
      " her: 0.0011\n",
      " Mrs: 0.0495\n",
      " from: 0.0457\n",
      "ed: 0.0172\n",
      " too: 0.0127\n",
      " a: 0.0123\n",
      " of: 0.1335\n",
      ".: 0.0838\n",
      " by: 0.0621\n",
      "--: 0.0474\n",
      " it: 0.0342\n",
      " the: 0.8396\n",
      " my: 0.0198\n",
      " a: 0.0167\n",
      " his: 0.0128\n",
      " their: 0.0089\n",
      " deep: 0.2557\n",
      " face: 0.0153\n",
      " picture: 0.0152\n",
      " Riv: 0.0106\n",
      " moment: 0.0104\n",
      " arm: 0.6602\n",
      " down: 0.0240\n",
      " that: 0.0179\n",
      ".: 0.0137\n",
      ",: 0.0096\n",
      "-: 0.7611\n",
      " of: 0.0137\n",
      " through: 0.0060\n",
      " told: 0.0051\n",
      " lifted: 0.0037\n",
      "chairs: 0.5533\n",
      "rooms: 0.0256\n",
      "hum: 0.0236\n",
      "piece: 0.0128\n",
      "cl: 0.0113\n",
      "-: 0.0618\n",
      " on: 0.0426\n",
      " me: 0.0256\n",
      " of: 0.0240\n",
      " fellow: 0.0189\n",
      ".: 0.2083\n",
      " last: 0.0498\n",
      "anim: 0.0323\n",
      " enough: 0.0276\n",
      " of: 0.0260\n",
      ":: 0.8842\n",
      " that: 0.0060\n",
      "--: 0.0049\n",
      ".: 0.0022\n",
      " word: 0.0019\n",
      " \": 0.9563\n",
      " the: 0.0026\n",
      " no: 0.0017\n",
      " they: 0.0016\n",
      " so: 0.0014\n",
      "Yes: 0.7437\n",
      "G: 0.0071\n",
      "strong: 0.0057\n",
      "There: 0.0054\n",
      "Be: 0.0047\n",
      "--: 0.9480\n",
      ",: 0.0295\n",
      " of: 0.0020\n",
      " about: 0.0008\n",
      " was: 0.0005\n",
      "she: 0.4201\n",
      "and: 0.2880\n",
      "that: 0.0144\n",
      "his: 0.0134\n",
      "or: 0.0068\n",
      " a: 0.0753\n",
      "'s: 0.0651\n",
      " thought: 0.0301\n",
      " m: 0.0216\n",
      " was: 0.0207\n",
      " fact: 0.0937\n",
      " an: 0.0500\n",
      " that: 0.0482\n",
      " and: 0.0204\n",
      ",: 0.0170\n",
      "!: 0.2375\n",
      "ache: 0.0315\n",
      " herself: 0.0277\n",
      " awful: 0.0213\n",
      "!\": 0.0140\n",
      " here: 0.0403\n",
      " else: 0.0370\n",
      " simple: 0.0284\n",
      ",: 0.0283\n",
      " fact: 0.0232\n",
      " women: 0.0783\n",
      " an: 0.0395\n",
      "\n",
      ": 0.0180\n",
      " something: 0.0159\n",
      " are: 0.0138\n",
      " about: 0.9245\n",
      ",: 0.0067\n",
      " to: 0.0023\n",
      " the: 0.0019\n",
      " him: 0.0011\n",
      " her: 0.8172\n",
      " the: 0.0508\n",
      " that: 0.0114\n",
      " to: 0.0071\n",
      " my: 0.0036\n",
      ",: 0.1660\n",
      " to: 0.0814\n",
      " him: 0.0630\n",
      " again: 0.0527\n",
      " what: 0.0283\n",
      " he: 0.3578\n",
      " a: 0.2414\n",
      " she: 0.0616\n",
      " I: 0.0392\n",
      " exc: 0.0176\n",
      " and: 0.4687\n",
      " he: 0.0282\n",
      " the: 0.0214\n",
      " degree: 0.0197\n",
      ".\": 0.0155\n",
      " he: 0.2188\n",
      " of: 0.1236\n",
      " it: 0.0817\n",
      "'s: 0.0802\n",
      ",: 0.0454\n",
      " me: 0.2283\n",
      " of: 0.0460\n",
      " back: 0.0458\n",
      " might: 0.0219\n",
      "--: 0.0185\n",
      " his: 0.6771\n",
      " the: 0.2475\n",
      " from: 0.0062\n",
      " a: 0.0050\n",
      " that: 0.0035\n",
      " glory: 0.3514\n",
      " head: 0.2343\n",
      " _: 0.0096\n",
      " eyes: 0.0073\n",
      " wife: 0.0072\n",
      " you: 0.6599\n",
      " me: 0.0244\n",
      " at: 0.0189\n",
      " them: 0.0085\n",
      "--: 0.0080\n",
      ",: 0.0876\n",
      " at: 0.0850\n",
      ".\": 0.0749\n",
      " into: 0.0435\n",
      "_: 0.0412\n",
      ",: 0.0532\n",
      " him: 0.0518\n",
      " my: 0.0394\n",
      " one: 0.0367\n",
      " idea: 0.0362\n",
      " \": 0.0370\n",
      "ains: 0.0317\n",
      " at: 0.0197\n",
      " once: 0.0180\n",
      ";: 0.0122\n",
      "if: 0.2836\n",
      "and: 0.0748\n",
      "she: 0.0448\n",
      "as: 0.0294\n",
      "it: 0.0161\n",
      "ess: 0.2886\n",
      "'s: 0.0689\n",
      " had: 0.0295\n",
      " as: 0.0150\n",
      " have: 0.0141\n",
      "'t: 0.0577\n",
      " his: 0.0538\n",
      " aside: 0.0314\n",
      " quality: 0.0284\n",
      " him: 0.0278\n",
      " _: 0.1200\n",
      " know: 0.0327\n",
      " see: 0.0294\n",
      " say: 0.0267\n",
      " let: 0.0233\n",
      " knew: 0.2183\n",
      " had: 0.1486\n",
      " _: 0.0274\n",
      " having: 0.0263\n",
      " looking: 0.0180\n",
      " me: 0.1033\n",
      " a: 0.0854\n",
      ".\": 0.0692\n",
      " the: 0.0639\n",
      "?: 0.0594\n",
      " and: 0.2243\n",
      " you: 0.1195\n",
      " it: 0.0444\n",
      " as: 0.0398\n",
      " when: 0.0338\n",
      " his: 0.2297\n",
      " the: 0.1909\n",
      " were: 0.0312\n",
      " heard: 0.0163\n",
      " Jack: 0.0147\n",
      " than: 0.0601\n",
      ",: 0.0503\n",
      " on: 0.0494\n",
      " room: 0.0342\n",
      " he: 0.0207\n",
      ",\": 0.1456\n",
      ",: 0.0959\n",
      " is: 0.0664\n",
      " on: 0.0633\n",
      " at: 0.0572\n",
      " when: 0.1889\n",
      " in: 0.1257\n",
      ",: 0.0416\n",
      " says: 0.0262\n",
      " aside: 0.0194\n",
      " I: 0.9713\n",
      " he: 0.0078\n",
      " she: 0.0023\n",
      " his: 0.0008\n",
      " in: 0.0006\n",
      "Every effort moves you know you meant to do recovering let of the deep arm-aStud laugh: \"Yes-- rainThe constraint Enterprisearro something about n if clearMoney leading back his tell brushmanagedLiverpool-- hostDon't ever knew, passingrimination himself moved when I\n"
     ]
    }
   ],
   "source": [
    "generate_and_print_with_decoding(gpt, tokenizer, torch_device, \"Every effort moves you\", print_top_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you in the inevitable garlanded frame. The mere outline of the frame called up all Gisburn's an awful simpleton, you know, when, that the window-curtains, I had the secret, and were amusing himself by holding\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "generate_and_print_with_decoding(gpt, tokenizer, torch_device, \"Every effort moves you\", partial(random, temperature=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk(model, token_ids, max_new_tokens, context_size, temperature=1.0, k=5):\n",
    "    logits = None\n",
    "    if temperature == 0:\n",
    "        return greedy(model, token_ids, max_new_tokens, context_size)\n",
    "    for i in range(max_new_tokens):\n",
    "        context_token_ids = token_ids[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(context_token_ids)\n",
    "\n",
    "        logits = logits[:, -1, :]\n",
    "        scaled_logits = logits/temperature\n",
    "        _, inds = torch.topk(scaled_logits, dim=-1, k=k)\n",
    "        mask = torch.ones_like(scaled_logits, dtype=torch.bool).scatter_(-1, inds, False)\n",
    "        topk_scaled_logits = scaled_logits.masked_fill_(mask, -torch.inf)\n",
    "        probas = torch.softmax(topk_scaled_logits, dim=-1)\n",
    "        token_id_next = torch.multinomial(probas, num_samples=1)  # Pure Random\n",
    "        token_ids = torch.cat((token_ids, token_id_next), dim=1) \n",
    "\n",
    "    return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = torch.rand((10, 10))\n",
    "# vals, inds = torch.topk(temp, dim=-1, k=3)\n",
    "# mask = torch.ones_like(temp, dtype=torch.bool).scatter_(-1, inds, False)\n",
    "# print(mask)\n",
    "# topk_temp = temp.masked_fill_(mask, -torch.inf)\n",
    "# torch.softmax(topk_temp, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n"
     ]
    }
   ],
   "source": [
    "generate_and_print_with_decoding(gpt, tokenizer, torch_device, \"Every effort moves you\", partial(topk, temperature=0.5, k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minp(model, token_ids, max_new_tokens, context_size, temperature=1.0, minp=0.9):\n",
    "    logits = None\n",
    "    if temperature == 0:\n",
    "        return greedy(model, token_ids, max_new_tokens, context_size)\n",
    "    for i in range(max_new_tokens):\n",
    "        context_token_ids = token_ids[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(context_token_ids)\n",
    "\n",
    "        logits = logits[:, -1, :]\n",
    "        probas = torch.softmax(logits/temperature, dim=-1)\n",
    "        sorted_probas, sorted_indices = torch.sort(probas, descending=True, dim=-1)\n",
    "        cumulative_probas = torch.cumsum(sorted_probas, dim=-1)\n",
    "        minp_mask = cumulative_probas > minp\n",
    "        unsorted_minp_mask = torch.zeros_like(probas, dtype=torch.bool).scatter_(-1, sorted_indices, minp_mask)\n",
    "        sum_prob = torch.sum(sorted_probas.masked_fill_(minp_mask, 0), dim=-1)\n",
    "        if sum_prob == 0:\n",
    "            token_id_next = torch.argmax(probas, dim=-1, keepdim=True)  # Pure Greed\n",
    "            token_ids = torch.cat((token_ids, token_id_next), dim=1) \n",
    "        else:\n",
    "            minp_probas = probas.masked_fill_(unsorted_minp_mask, 0)/sum_prob\n",
    "            token_id_next = torch.multinomial(minp_probas, num_samples=1)  # Pure Random\n",
    "            token_ids = torch.cat((token_ids, token_id_next), dim=1) \n",
    "\n",
    "    return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you surface to Gisburn rather a cheap predicted--though a good faith?\" I half should my have fore surprise to me to hair that, with the drawing up his placed them at my most--see Occupations of looking cleverer than he was.\n"
     ]
    }
   ],
   "source": [
    "generate_and_print_with_decoding(gpt, tokenizer, torch_device, \"Every effort moves you\", partial(minp, temperature=1.0, minp=0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gpt.state_dict(), \"gpt-trained.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_g/9l_52ymn66vdszd0ryvbb0640000gp/T/ipykernel_24346/3928202563.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_gpt_state = torch.load(\"gpt-trained.pth\", map_location=torch_device)\n"
     ]
    }
   ],
   "source": [
    "loaded_gpt_state = torch.load(\"gpt-trained.pth\", map_location=torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_gpt_model = GPTModel(GPT_CONFIG_124M).to(torch_device)\n",
    "loaded_gpt_model.load_state_dict(loaded_gpt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_gpt_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled = torch.compile(loaded_gpt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "BackendCompilerFailed",
     "evalue": "backend='inductor' raised:\nLoweringException: TypeError: 'NoneType' object is not callable\n  target: aten.mean.dim\n  args[0]: TensorBox(StorageBox(\n    Pointwise(\n      'mps',\n      torch.float32,\n      def inner_fn(index):\n          _, i1, i2 = index\n          tmp0 = ops.load(arg0_1, i1)\n          tmp1 = ops.load(arg1_1, i2 + 768 * tmp0)\n          tmp2 = ops.index_expr(i1, dtype=torch.int64)\n          tmp3 = ops.load(arg2_1, i2 + 768 * tmp2)\n          tmp4 = tmp1 + tmp3\n          return tmp4\n      ,\n      ranges=[1, 4, 768],\n      origin_node=add,\n      origins=OrderedSet([iota, embedding_1, embedding, add])\n    )\n  ))\n  args[1]: [-1]\n  args[2]: True\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLoweringException\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:1446\u001b[0m, in \u001b[0;36mOutputGraph._call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     compiler_fn \u001b[38;5;241m=\u001b[39m WrapperBackend(compiler_fn)\n\u001b[0;32m-> 1446\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexample_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1447\u001b[0m _step_logger()(logging\u001b[38;5;241m.\u001b[39mINFO, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone compiler function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_dynamo/repro/after_dynamo.py:129\u001b[0m, in \u001b[0;36mWrapBackendDebug.__call__\u001b[0;34m(self, gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m     compiled_gm \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_gm\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/__init__.py:2234\u001b[0m, in \u001b[0;36m_TorchCompileInductorWrapper.__call__\u001b[0;34m(self, model_, inputs_)\u001b[0m\n\u001b[1;32m   2232\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inductor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompile_fx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compile_fx\n\u001b[0;32m-> 2234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompile_fx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_patches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:1521\u001b[0m, in \u001b[0;36mcompile_fx\u001b[0;34m(model_, example_inputs_, inner_compile, config_patches, decompositions)\u001b[0m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m V\u001b[38;5;241m.\u001b[39mset_fake_mode(fake_mode), torch\u001b[38;5;241m.\u001b[39m_guards\u001b[38;5;241m.\u001b[39mtracing(\n\u001b[1;32m   1517\u001b[0m     tracing_context\n\u001b[1;32m   1518\u001b[0m ), compiled_autograd\u001b[38;5;241m.\u001b[39mdisable(), functorch_config\u001b[38;5;241m.\u001b[39mpatch(\n\u001b[1;32m   1519\u001b[0m     unlift_effect_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1520\u001b[0m ):\n\u001b[0;32m-> 1521\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43maot_autograd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfw_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbw_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbw_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1524\u001b[0m \u001b[43m        \u001b[49m\u001b[43minference_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minference_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecompositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecompositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartition_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_inference_input_mutations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcudagraphs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcudagraphs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_dynamo/backends/common.py:72\u001b[0m, in \u001b[0;36mAotAutograd.__call__\u001b[0;34m(self, gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m enable_aot_logging(), patch_config:\n\u001b[0;32m---> 72\u001b[0m     cg \u001b[38;5;241m=\u001b[39m \u001b[43maot_module_simplified\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot_autograd\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:1071\u001b[0m, in \u001b[0;36maot_module_simplified\u001b[0;34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, keep_inference_input_mutations, inference_compiler, cudagraphs)\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1071\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mdispatch_and_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mGmWrapper):\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;66;03m# This function is called by the flatten_graph_inputs wrapper, which boxes\u001b[39;00m\n\u001b[1;32m   1075\u001b[0m     \u001b[38;5;66;03m# the inputs so that they can be freed before the end of this scope.\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m     \u001b[38;5;66;03m# For overhead reasons, this is not the default wrapper, see comment:\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/122535/files#r1560096481\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:1056\u001b[0m, in \u001b[0;36maot_module_simplified.<locals>.dispatch_and_compile\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compiled_autograd\u001b[38;5;241m.\u001b[39mdisable():\n\u001b[0;32m-> 1056\u001b[0m     compiled_fn, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_aot_dispatcher_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunctional_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1059\u001b[0m \u001b[43m        \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfake_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:522\u001b[0m, in \u001b[0;36mcreate_aot_dispatcher_function\u001b[0;34m(flat_fn, fake_flat_args, aot_config, fake_mode, shape_env)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_aot_dispatcher_function\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 522\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_create_aot_dispatcher_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape_env\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:759\u001b[0m, in \u001b[0;36m_create_aot_dispatcher_function\u001b[0;34m(flat_fn, fake_flat_args, aot_config, fake_mode, shape_env)\u001b[0m\n\u001b[1;32m    757\u001b[0m compiler_fn \u001b[38;5;241m=\u001b[39m choose_dispatcher(needs_autograd, aot_config)\n\u001b[0;32m--> 759\u001b[0m compiled_fn, fw_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_dup_fake_script_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[43m    \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfw_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn, fw_metadata\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:179\u001b[0m, in \u001b[0;36maot_dispatch_base\u001b[0;34m(flat_fn, flat_args, aot_config, fw_metadata)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m TracingContext\u001b[38;5;241m.\u001b[39mreport_output_strides() \u001b[38;5;28;01mas\u001b[39;00m fwd_output_strides:\n\u001b[0;32m--> 179\u001b[0m     compiled_fw \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfw_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdated_flat_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fakified_out_wrapper\u001b[38;5;241m.\u001b[39mneeds_post_compile:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:1350\u001b[0m, in \u001b[0;36mcompile_fx.<locals>.fw_compiler_base\u001b[0;34m(model, example_inputs, is_inference)\u001b[0m\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dynamo_utils\u001b[38;5;241m.\u001b[39mdynamo_timed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompile_fx.<locals>.fw_compiler_base\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1350\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_fw_compiler_base\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_inference\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:1421\u001b[0m, in \u001b[0;36mcompile_fx.<locals>._fw_compiler_base\u001b[0;34m(model, example_inputs, is_inference)\u001b[0m\n\u001b[1;32m   1413\u001b[0m     user_visible_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39mfromkeys(\n\u001b[1;32m   1414\u001b[0m         n\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m   1415\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m model_outputs[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(n, torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mNode)\n\u001b[1;32m   1419\u001b[0m     )\n\u001b[0;32m-> 1421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatic_input_idxs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_static_input_idxs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfixed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcudagraphs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcudagraphs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_inference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_inference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mboxed_forward_device_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforward_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1429\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_visible_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_visible_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1430\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:475\u001b[0m, in \u001b[0;36mcompile_fx_inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m stack\u001b[38;5;241m.\u001b[39menter_context(DebugContext())\n\u001b[0;32m--> 475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrap_compiler_debug\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_compile_fx_inner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompiler_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minductor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_dynamo/repro/after_aot.py:85\u001b[0m, in \u001b[0;36mwrap_compiler_debug.<locals>.debug_wrapper\u001b[0;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# Call the compiler_fn - which is either aot_autograd or inductor\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# with fake inputs\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m     inner_compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# TODO: Failures here are troublesome because no real inputs,\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# need a different serialization strategy\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:661\u001b[0m, in \u001b[0;36m_compile_fx_inner\u001b[0;34m(gm, example_inputs, cudagraphs, static_input_idxs, is_backward, graph_id, cpp_wrapper, aot_mode, is_inference, boxed_forward_device_index, user_visible_outputs, layout_opt, extern_node_serializer)\u001b[0m\n\u001b[1;32m    659\u001b[0m             \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39m_is_inductor_static \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m--> 661\u001b[0m     compiled_graph \u001b[38;5;241m=\u001b[39m \u001b[43mFxGraphCache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcodegen_and_compile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgraph_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_to_check\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfx_graph_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfx_graph_remote_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_inductor/codecache.py:1334\u001b[0m, in \u001b[0;36mFxGraphCache.load\u001b[0;34m(compile_fx_fn, gm, example_inputs, fx_kwargs, inputs_to_check, local, remote)\u001b[0m\n\u001b[1;32m   1333\u001b[0m cache_event_time \u001b[38;5;241m=\u001b[39m start_time\n\u001b[0;32m-> 1334\u001b[0m compiled_graph \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_fx_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_to_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfx_kwargs\u001b[49m\n\u001b[1;32m   1336\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1337\u001b[0m compiled_graph\u001b[38;5;241m.\u001b[39m_time_taken_ns \u001b[38;5;241m=\u001b[39m time_ns() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:570\u001b[0m, in \u001b[0;36m_compile_fx_inner.<locals>.codegen_and_compile\u001b[0;34m(gm, example_inputs, inputs_to_check, fx_kwargs)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;124;03mThis function calls fx_codegen_and_compile and also adds some extra metadata to the resulting\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;124;03mcompiled fx graph. The metadata is saved to FXGraphCache.\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m compiled_graph \u001b[38;5;241m=\u001b[39m \u001b[43mfx_codegen_and_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfx_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(compiled_graph, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;66;03m# We only return a string in aot mode, in which case we don't\u001b[39;00m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# need to do any post-compilation steps: we just return the string,\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;66;03m# which is the filename of the compiled code.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:859\u001b[0m, in \u001b[0;36mfx_codegen_and_compile\u001b[0;34m(gm, example_inputs, cudagraphs, static_input_idxs, is_backward, graph_id, cpp_wrapper, aot_mode, is_inference, user_visible_outputs, layout_opt, extern_node_serializer)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m V\u001b[38;5;241m.\u001b[39mset_graph_handler(graph):\n\u001b[0;32m--> 859\u001b[0m     \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    860\u001b[0m     output_strides: List[Optional[Tuple[_StrideExprStr, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_inductor/graph.py:780\u001b[0m, in \u001b[0;36mGraphLowering.run\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGraphLowering.run\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/fx/interpreter.py:146\u001b[0m, in \u001b[0;36mInterpreter.run\u001b[0;34m(self, initial_env, enable_io_processing, *args)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv[node] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_inductor/graph.py:1319\u001b[0m, in \u001b[0;36mGraphLowering.run_node\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1318\u001b[0m     debug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1319\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;66;03m# require the same stride order for dense outputs,\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;66;03m# 1. user-land view() will not throw because inductor\u001b[39;00m\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;66;03m# output different strides than eager\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;66;03m# 2: as_strided ops, we need make sure its input has same size/stride with\u001b[39;00m\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;66;03m# eager model to align with eager behavior.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/fx/interpreter.py:203\u001b[0m, in \u001b[0;36mInterpreter.run_node\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(kwargs, \u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m--> 203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_inductor/graph.py:1024\u001b[0m, in \u001b[0;36mGraphLowering.call_function\u001b[0;34m(self, target, args, kwargs)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LoweringException(e, target, args, kwargs)\u001b[38;5;241m.\u001b[39mwith_traceback(\n\u001b[1;32m   1025\u001b[0m         e\u001b[38;5;241m.\u001b[39m__traceback__\n\u001b[1;32m   1026\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_inductor/graph.py:1021\u001b[0m, in \u001b[0;36mGraphLowering.call_function\u001b[0;34m(self, target, args, kwargs)\u001b[0m\n\u001b[1;32m   1020\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  via \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, lowerings[target])  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[0;32m-> 1021\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mlowerings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_inductor/lowering.py:361\u001b[0m, in \u001b[0;36m_register_lowering.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    359\u001b[0m     args \u001b[38;5;241m=\u001b[39m [args]\n\u001b[0;32m--> 361\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mdecomp_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m validate_ir(out)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_inductor/lowering.py:5143\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(x, axis, keepdim, dtype)\u001b[0m\n\u001b[1;32m   5142\u001b[0m     x \u001b[38;5;241m=\u001b[39m to_dtype(x, torch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[0;32m-> 5143\u001b[0m sum_result \u001b[38;5;241m=\u001b[39m \u001b[43msum_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5144\u001b[0m denom \u001b[38;5;241m=\u001b[39m sympy_product(size[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m axis)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_inductor/lowering.py:361\u001b[0m, in \u001b[0;36m_register_lowering.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    359\u001b[0m     args \u001b[38;5;241m=\u001b[39m [args]\n\u001b[0;32m--> 361\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mdecomp_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m validate_ir(out)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_inductor/lowering.py:5534\u001b[0m, in \u001b[0;36msum_\u001b[0;34m(x, axis, keepdims, dtype)\u001b[0m\n\u001b[1;32m   5533\u001b[0m fn \u001b[38;5;241m=\u001b[39m make_reduction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m, override_return_dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m-> 5534\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_inductor/lowering.py:5108\u001b[0m, in \u001b[0;36mmake_reduction.<locals>.inner\u001b[0;34m(x, axis, keepdims, dtype)\u001b[0m\n\u001b[1;32m   5101\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m _make_reduction_inner(\n\u001b[1;32m   5102\u001b[0m     x,\n\u001b[1;32m   5103\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5106\u001b[0m     override_return_dtype\u001b[38;5;241m=\u001b[39moverride_return_dtype,\n\u001b[1;32m   5107\u001b[0m )\n\u001b[0;32m-> 5108\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mReduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduction_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_node\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   5110\u001b[0m     result\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mdata, Reduction\n\u001b[1;32m   5111\u001b[0m ):  \u001b[38;5;66;03m# Only realize if reduction isn't unrolled\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_inductor/ir.py:1179\u001b[0m, in \u001b[0;36mReduction.create\u001b[0;34m(cls, device, dst_dtype, src_dtype, inner_fn, ranges, reduction_ranges, reduction_type, reduction_hint, input_node)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;66;03m# triton doesn't support reduce to single element well, so break it up\u001b[39;00m\n\u001b[0;32m-> 1179\u001b[0m hint, split \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_splits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdst_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrc_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m    \u001b[49m\u001b[43minner_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mranges\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreduction_ranges\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreduction_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreduction_numel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_node\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# intermediate reduction in split can contain complex indexing,\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# and num_splits will fail to correctly set the hint\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;66;03m# reuse the passed hint if available\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_inductor/ir.py:851\u001b[0m, in \u001b[0;36mReduction.num_splits\u001b[0;34m(device, dst_dtype, src_dtype, inner_fn, ranges, reduction_ranges, reduction_type, reduction_numel, input_node)\u001b[0m\n\u001b[1;32m    848\u001b[0m numel_hint \u001b[38;5;241m=\u001b[39m V\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39msizevars\u001b[38;5;241m.\u001b[39msymbolic_hint(sympy_product(ranges))\n\u001b[1;32m    850\u001b[0m should_split \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 851\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mV\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBackendFeature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREDUCE_TO_SINGLE_ELEMENT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m reduction_type\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m    854\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margmax\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margmin\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    856\u001b[0m     )\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m config\u001b[38;5;241m.\u001b[39msplit_reductions\n\u001b[1;32m    858\u001b[0m     \u001b[38;5;66;03m# We don't support unbacked symints\u001b[39;00m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m _is_static(reduction_numel_hint)\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m _is_static(numel_hint)\n\u001b[1;32m    861\u001b[0m )\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_split:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_inductor/graph.py:465\u001b[0m, in \u001b[0;36mGraphLowering.has_feature\u001b[0;34m(self, device, feature)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(feature, BackendFeature), feature\n\u001b[0;32m--> 465\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_backend_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_device_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_inductor/codegen/common.py:170\u001b[0m, in \u001b[0;36mget_backend_features\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    169\u001b[0m scheduling \u001b[38;5;241m=\u001b[39m get_scheduling_for_device(device_type)\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscheduling\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget_backend_features(device)\n",
      "\u001b[0;31mLoweringException\u001b[0m: TypeError: 'NoneType' object is not callable\n  target: aten.mean.dim\n  args[0]: TensorBox(StorageBox(\n    Pointwise(\n      'mps',\n      torch.float32,\n      def inner_fn(index):\n          _, i1, i2 = index\n          tmp0 = ops.load(arg0_1, i1)\n          tmp1 = ops.load(arg1_1, i2 + 768 * tmp0)\n          tmp2 = ops.index_expr(i1, dtype=torch.int64)\n          tmp3 = ops.load(arg2_1, i2 + 768 * tmp2)\n          tmp4 = tmp1 + tmp3\n          return tmp4\n      ,\n      ranges=[1, 4, 768],\n      origin_node=add,\n      origins=OrderedSet([iota, embedding_1, embedding, add])\n    )\n  ))\n  args[1]: [-1]\n  args[2]: True",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgenerate_and_print_with_decoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompiled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvery effort moves you\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgreedy\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 20\u001b[0m, in \u001b[0;36mgenerate_and_print_with_decoding\u001b[0;34m(model, tokenizer, device, start_context, decoding_fn)\u001b[0m\n\u001b[1;32m     18\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m text_to_token_ids(start_context, tokenizer)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 20\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m \u001b[43mdecoding_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m decoded_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(token_ids[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(decoded_text\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[0;32mIn[23], line 6\u001b[0m, in \u001b[0;36mgreedy\u001b[0;34m(model, token_ids, max_new_tokens, context_size)\u001b[0m\n\u001b[1;32m      4\u001b[0m context_token_ids \u001b[38;5;241m=\u001b[39m token_ids[:, \u001b[38;5;241m-\u001b[39mcontext_size:]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 6\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext_token_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m      9\u001b[0m probas \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:465\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m saved_dynamic_layer_stack_depth \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    461\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mget_dynamic_layer_stack_depth()\n\u001b[1;32m    462\u001b[0m )\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[1;32m    468\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mpop_dynamic_layer_stack_and_undo_to_depth(\n\u001b[1;32m    469\u001b[0m         saved_dynamic_layer_stack_depth\n\u001b[1;32m    470\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:1269\u001b[0m, in \u001b[0;36mCatchErrorsWrapper.__call__\u001b[0;34m(self, frame, cache_entry, frame_state)\u001b[0m\n\u001b[1;32m   1263\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m hijacked_callback(\n\u001b[1;32m   1264\u001b[0m                 frame, cache_entry, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks, frame_state\n\u001b[1;32m   1265\u001b[0m             )\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_lock, _disable_current_modes():\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;66;03m# skip=1: skip this frame\u001b[39;00m\n\u001b[0;32m-> 1269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m   1271\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:1064\u001b[0m, in \u001b[0;36mConvertFrame.__call__\u001b[0;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m   1062\u001b[0m counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1064\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inner_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1067\u001b[0m     counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1068\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:526\u001b[0m, in \u001b[0;36mConvertFrameAssert.__call__\u001b[0;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m    510\u001b[0m compile_id \u001b[38;5;241m=\u001b[39m CompileId(frame_id, frame_compile_id)\n\u001b[1;32m    512\u001b[0m signpost_event(\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_convert_frame_assert._compile\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m     },\n\u001b[1;32m    524\u001b[0m )\n\u001b[0;32m--> 526\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_globals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_locals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_builtins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_one_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompile_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompile_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:924\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_entry, cache_size, frame, frame_state, compile_id, skip)\u001b[0m\n\u001b[1;32m    922\u001b[0m guarded_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 924\u001b[0m     guarded_code \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m guarded_code\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:666\u001b[0m, in \u001b[0;36m_compile.<locals>.compile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_compile.compile_inner\u001b[39m\u001b[38;5;124m\"\u001b[39m, phase_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentire_frame_compile\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    665\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m CompileTimeInstructionCounter\u001b[38;5;241m.\u001b[39mrecord():\n\u001b[0;32m--> 666\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_utils_internal.py:87\u001b[0m, in \u001b[0;36mcompile_time_strobelight_meta.<locals>.compile_time_strobelight_meta_inner.<locals>.wrapper_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m StrobelightCompileTimeProfiler\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StrobelightCompileTimeProfiler\u001b[38;5;241m.\u001b[39mprofile_compile_time(\n\u001b[1;32m     90\u001b[0m     function, phase_name, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     91\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:699\u001b[0m, in \u001b[0;36m_compile.<locals>._compile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    697\u001b[0m CompileContext\u001b[38;5;241m.\u001b[39mget()\u001b[38;5;241m.\u001b[39mattempt \u001b[38;5;241m=\u001b[39m attempt\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 699\u001b[0m     out_code \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_code_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mRestartAnalysis \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_dynamo/bytecode_transformation.py:1322\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m   1319\u001b[0m instructions \u001b[38;5;241m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[1;32m   1320\u001b[0m propagate_line_nums(instructions)\n\u001b[0;32m-> 1322\u001b[0m \u001b[43mtransformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:219\u001b[0m, in \u001b[0;36mpreserve_global_state.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m exit_stack\u001b[38;5;241m.\u001b[39menter_context(\n\u001b[1;32m    216\u001b[0m     torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39m_symbolic_trace\u001b[38;5;241m.\u001b[39m_maybe_revert_all_patches()\n\u001b[1;32m    217\u001b[0m )\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     cleanup\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:634\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracing(tracer\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mtracing_context), tracer\u001b[38;5;241m.\u001b[39mset_current_tx():\n\u001b[0;32m--> 634\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mUnspecializeRestartAnalysis:\n\u001b[1;32m    636\u001b[0m     speculation_log\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:2796\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 2796\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:983\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 983\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    984\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:895\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 895\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopcode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:2987\u001b[0m, in \u001b[0;36mInstructionTranslator.RETURN_VALUE\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   2986\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mRETURN_VALUE\u001b[39m(\u001b[38;5;28mself\u001b[39m, inst):\n\u001b[0;32m-> 2987\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:2972\u001b[0m, in \u001b[0;36mInstructionTranslator._return\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   2967\u001b[0m _step_logger()(\n\u001b[1;32m   2968\u001b[0m     logging\u001b[38;5;241m.\u001b[39mINFO,\n\u001b[1;32m   2969\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchdynamo done tracing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minst\u001b[38;5;241m.\u001b[39mopname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2970\u001b[0m )\n\u001b[1;32m   2971\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m triggered compile\u001b[39m\u001b[38;5;124m\"\u001b[39m, inst\u001b[38;5;241m.\u001b[39mopname)\n\u001b[0;32m-> 2972\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_subgraph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2973\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreason\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGraphCompileReason\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2975\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreturn_value\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_break\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   2976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2977\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2978\u001b[0m return_inst \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2979\u001b[0m     create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2980\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inst\u001b[38;5;241m.\u001b[39mopname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2981\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_CONST\u001b[39m\u001b[38;5;124m\"\u001b[39m, argval\u001b[38;5;241m=\u001b[39minst\u001b[38;5;241m.\u001b[39margval)\n\u001b[1;32m   2982\u001b[0m )\n\u001b[1;32m   2983\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39madd_output_instructions([return_inst])\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:1117\u001b[0m, in \u001b[0;36mOutputGraph.compile_subgraph\u001b[0;34m(self, tx, partial_convert, reason)\u001b[0m\n\u001b[1;32m   1114\u001b[0m append_prefix_insts()\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;66;03m# optimization to generate better code in a common case\u001b[39;00m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_output_instructions(\n\u001b[0;32m-> 1117\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_and_call_fx_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mreversed\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstack_values\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;241m+\u001b[39m [create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUNPACK_SEQUENCE\u001b[39m\u001b[38;5;124m\"\u001b[39m, arg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(stack_values))]\n\u001b[1;32m   1119\u001b[0m )\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;66;03m# restore all the live local vars\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_output_instructions(\n\u001b[1;32m   1122\u001b[0m     [PyCodegen(tx)\u001b[38;5;241m.\u001b[39mcreate_store(var) \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(restore_vars)]\n\u001b[1;32m   1123\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:1369\u001b[0m, in \u001b[0;36mOutputGraph.compile_and_call_fx_graph\u001b[0;34m(self, tx, rv, root)\u001b[0m\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracing_context\u001b[38;5;241m.\u001b[39mfake_mode \u001b[38;5;241m=\u001b[39m backend_fake_mode\n\u001b[1;32m   1368\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore_global_state():\n\u001b[0;32m-> 1369\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_user_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lazy_graph_module\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _LazyGraphModule\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(compiled_fn, _LazyGraphModule) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(compiled_fn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__self__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), _LazyGraphModule)\n\u001b[1;32m   1375\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m compiled_fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_lazy_forward\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;66;03m# this is a _LazyGraphModule. This makes it easier for dynamo to\u001b[39;00m\n\u001b[1;32m   1380\u001b[0m     \u001b[38;5;66;03m# optimize a _LazyGraphModule.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:1416\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m   1412\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_user_compiler\u001b[39m(\u001b[38;5;28mself\u001b[39m, gm: fx\u001b[38;5;241m.\u001b[39mGraphModule) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CompiledFn:\n\u001b[1;32m   1413\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[1;32m   1414\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutputGraph.call_user_compiler\u001b[39m\u001b[38;5;124m\"\u001b[39m, phase_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackend_compile\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1415\u001b[0m     ):\n\u001b[0;32m-> 1416\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_user_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/torch/_dynamo/output_graph.py:1465\u001b[0m, in \u001b[0;36mOutputGraph._call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m   1463\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1465\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BackendCompilerFailed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompiler_fn, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1467\u001b[0m signpost_event(\n\u001b[1;32m   1468\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1469\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutputGraph.call_user_compiler\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1475\u001b[0m     },\n\u001b[1;32m   1476\u001b[0m )\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn\n",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m: backend='inductor' raised:\nLoweringException: TypeError: 'NoneType' object is not callable\n  target: aten.mean.dim\n  args[0]: TensorBox(StorageBox(\n    Pointwise(\n      'mps',\n      torch.float32,\n      def inner_fn(index):\n          _, i1, i2 = index\n          tmp0 = ops.load(arg0_1, i1)\n          tmp1 = ops.load(arg1_1, i2 + 768 * tmp0)\n          tmp2 = ops.index_expr(i1, dtype=torch.int64)\n          tmp3 = ops.load(arg2_1, i2 + 768 * tmp2)\n          tmp4 = tmp1 + tmp3\n          return tmp4\n      ,\n      ranges=[1, 4, 768],\n      origin_node=add,\n      origins=OrderedSet([iota, embedding_1, embedding, add])\n    )\n  ))\n  args[1]: [-1]\n  args[2]: True\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
     ]
    }
   ],
   "source": [
    "generate_and_print_with_decoding(compiled, tokenizer, torch_device, \"Every effort moves you\", greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading from HF Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacobjensen/.pyenv/versions/3.11.4/envs/jejllm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "# Load the model\n",
    "hf_model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "# Extract the weights\n",
    "hf_model_state_dict = hf_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight torch.Size([50257, 768])\n",
      "transformer.wpe.weight torch.Size([1024, 768])\n",
      "transformer.h.0.ln_1.weight torch.Size([768])\n",
      "transformer.h.0.ln_1.bias torch.Size([768])\n",
      "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.0.ln_2.weight torch.Size([768])\n",
      "transformer.h.0.ln_2.bias torch.Size([768])\n",
      "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_1.weight torch.Size([768])\n",
      "transformer.h.1.ln_1.bias torch.Size([768])\n",
      "transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.1.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.1.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_2.weight torch.Size([768])\n",
      "transformer.h.1.ln_2.bias torch.Size([768])\n",
      "transformer.h.1.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.1.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.1.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.1.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_1.weight torch.Size([768])\n",
      "transformer.h.2.ln_1.bias torch.Size([768])\n",
      "transformer.h.2.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.2.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.2.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.2.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_2.weight torch.Size([768])\n",
      "transformer.h.2.ln_2.bias torch.Size([768])\n",
      "transformer.h.2.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.2.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.2.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.2.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_1.weight torch.Size([768])\n",
      "transformer.h.3.ln_1.bias torch.Size([768])\n",
      "transformer.h.3.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.3.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.3.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.3.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_2.weight torch.Size([768])\n",
      "transformer.h.3.ln_2.bias torch.Size([768])\n",
      "transformer.h.3.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.3.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.3.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.3.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_1.weight torch.Size([768])\n",
      "transformer.h.4.ln_1.bias torch.Size([768])\n",
      "transformer.h.4.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.4.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.4.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.4.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_2.weight torch.Size([768])\n",
      "transformer.h.4.ln_2.bias torch.Size([768])\n",
      "transformer.h.4.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.4.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.4.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.4.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_1.weight torch.Size([768])\n",
      "transformer.h.5.ln_1.bias torch.Size([768])\n",
      "transformer.h.5.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.5.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.5.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.5.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_2.weight torch.Size([768])\n",
      "transformer.h.5.ln_2.bias torch.Size([768])\n",
      "transformer.h.5.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.5.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.5.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.5.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_1.weight torch.Size([768])\n",
      "transformer.h.6.ln_1.bias torch.Size([768])\n",
      "transformer.h.6.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.6.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.6.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.6.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_2.weight torch.Size([768])\n",
      "transformer.h.6.ln_2.bias torch.Size([768])\n",
      "transformer.h.6.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.6.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.6.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.6.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_1.weight torch.Size([768])\n",
      "transformer.h.7.ln_1.bias torch.Size([768])\n",
      "transformer.h.7.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.7.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.7.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.7.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_2.weight torch.Size([768])\n",
      "transformer.h.7.ln_2.bias torch.Size([768])\n",
      "transformer.h.7.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.7.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.7.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.7.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_1.weight torch.Size([768])\n",
      "transformer.h.8.ln_1.bias torch.Size([768])\n",
      "transformer.h.8.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.8.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.8.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.8.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_2.weight torch.Size([768])\n",
      "transformer.h.8.ln_2.bias torch.Size([768])\n",
      "transformer.h.8.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.8.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.8.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.8.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_1.weight torch.Size([768])\n",
      "transformer.h.9.ln_1.bias torch.Size([768])\n",
      "transformer.h.9.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.9.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.9.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.9.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_2.weight torch.Size([768])\n",
      "transformer.h.9.ln_2.bias torch.Size([768])\n",
      "transformer.h.9.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.9.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.9.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.9.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_1.weight torch.Size([768])\n",
      "transformer.h.10.ln_1.bias torch.Size([768])\n",
      "transformer.h.10.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.10.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.10.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.10.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_2.weight torch.Size([768])\n",
      "transformer.h.10.ln_2.bias torch.Size([768])\n",
      "transformer.h.10.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.10.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.10.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.10.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_1.weight torch.Size([768])\n",
      "transformer.h.11.ln_1.bias torch.Size([768])\n",
      "transformer.h.11.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.11.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.11.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.11.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_2.weight torch.Size([768])\n",
      "transformer.h.11.ln_2.bias torch.Size([768])\n",
      "transformer.h.11.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.11.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.11.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.11.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.ln_f.weight torch.Size([768])\n",
      "transformer.ln_f.bias torch.Size([768])\n",
      "lm_head.weight torch.Size([50257, 768])\n",
      "124439808 163037184\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in hf_model.parameters())\n",
    "total_params_from_state = 0\n",
    "for k, v in hf_model_state_dict.items():\n",
    "    print(k, v.shape)\n",
    "    total_params_from_state += v.numel()\n",
    "print(total_params, total_params_from_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jejllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

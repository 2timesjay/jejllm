{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    torch_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=torch_device)\n",
    "    print (x)\n",
    "else:\n",
    "    torch_device = torch.device(\"cpu\")\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "EMBEDDING_DIM = 3\n",
    "CONTEXT_LENGTH = 4\n",
    "QKV_DIM = 2\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length=CONTEXT_LENGTH, dropout=0.5, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.w_q = nn.Linear(d_in, d_out, bias=qkv_bias).to(torch_device)\n",
    "        self.w_k = nn.Linear(d_in, d_out, bias=qkv_bias).to(torch_device)\n",
    "        self.w_v = nn.Linear(d_in, d_out, bias=qkv_bias).to(torch_device)\n",
    "        self.dropout = nn.Dropout(dropout).to(torch_device)\n",
    "        self.register_buffer(\n",
    "            'mask', \n",
    "            torch.triu(\n",
    "                torch.ones(context_length, context_length), \n",
    "                diagonal=1,\n",
    "            ).to(torch_device)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        num_tokens = x.shape[-2]\n",
    "        queries = self.w_q(x)\n",
    "        keys = self.w_k(x)\n",
    "        attn_scores = queries @ keys.transpose(-2, -1)\n",
    "        causal_attn_scores = attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        causal_attn_weights = torch.softmax(causal_attn_scores*(self.d_out**0.5), dim=-1)\n",
    "        causal_attn_weights = self.dropout(causal_attn_weights)\n",
    "        values = self.w_v(x)\n",
    "        context = causal_attn_weights @ values\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "EMBEDDING_DIM = 3\n",
    "CONTEXT_LENGTH = 4\n",
    "QKV_DIM = 2\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out//num_heads\n",
    "        self.w_q = nn.Linear(d_in, d_out, bias=qkv_bias).to(torch_device)\n",
    "        self.w_k = nn.Linear(d_in, d_out, bias=qkv_bias).to(torch_device)\n",
    "        self.w_v = nn.Linear(d_in, d_out, bias=qkv_bias).to(torch_device)\n",
    "        self.w_o = nn.Linear(d_out, d_out).to(torch_device)\n",
    "        # self.w_o = nn.Identity().to(torch_device)\n",
    "        self.dropout = nn.Dropout(dropout).to(torch_device)\n",
    "        self.register_buffer(\n",
    "            'mask', \n",
    "            torch.triu(\n",
    "                torch.ones(context_length, context_length), \n",
    "                diagonal=1,\n",
    "            ).to(torch_device)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.w_k(x)\n",
    "        queries = self.w_q(x)\n",
    "        values = self.w_v(x)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(-3, -2)\n",
    "        # TODO: KV Cache Optimization\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim).transpose(-3, -2)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim).transpose(-3, -2)\n",
    "        attn_scores = queries @ keys.transpose(-2, -1)\n",
    "        causal_attn_scores = attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        causal_attn_weights = torch.softmax(causal_attn_scores/(keys.shape[-1]**0.5), dim=-1)\n",
    "        causal_attn_weights = self.dropout(causal_attn_weights)\n",
    "        context = (causal_attn_weights @ values).transpose(-3, -2)\n",
    "        context = context.contiguous().view(b, num_tokens, self.num_heads*self.head_dim)\n",
    "        context = self.w_o(context)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(d_model))\n",
    "        self.shift = nn.Parameter(torch.zeros(d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True, unbiased=False)\n",
    "        x = (self.scale * (x - mean) / (std + self.eps)) + self.shift\n",
    "        return x\n",
    "\n",
    "\n",
    "# class LayerNorm(nn.Module):\n",
    "#     def __init__(self, emb_dim):\n",
    "#         super().__init__()\n",
    "#         self.eps = 1e-5\n",
    "#         self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "#         self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         mean = x.mean(dim=-1, keepdim=True)\n",
    "#         var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "#         norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "#         return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (\n",
    "            1 + torch.tanh(\n",
    "                torch.sqrt(torch.tensor(2.0/torch.pi, device=torch_device)) * (x  + 0.044715 * x**3)\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward (nn.Module):\n",
    "    def __init__(self, cfg): \n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], cfg[\"emb_dim\"]*4).to(torch_device),\n",
    "            GELU().to(torch_device),\n",
    "            nn.Linear(cfg[\"emb_dim\"]*4, cfg[\"emb_dim\"]).to(torch_device),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = LayerNorm(config[\"emb_dim\"]).to(torch_device)\n",
    "        self.attn = MultiHeadAttention(config[\"emb_dim\"], config[\"emb_dim\"], config[\"context_length\"], config[\"drop_rate\"], config[\"n_heads\"], config[\"qkv_bias\"]).to(torch_device)\n",
    "        self.drop = nn.Dropout(config[\"drop_rate\"]).to(torch_device)\n",
    "        self.ln2 = LayerNorm(config[\"emb_dim\"]).to(torch_device)\n",
    "        self.ff = FeedForward(config).to(torch_device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.drop(self.attn(self.ln1(x))) + x\n",
    "        x = self.drop(self.ff(self.ln2(x))) + x\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# class TransformerBlock(nn.Module):\n",
    "#     def __init__(self, cfg):\n",
    "#         super().__init__()\n",
    "#         self.att = MultiHeadAttention(\n",
    "#             d_in=cfg[\"emb_dim\"],\n",
    "#             d_out=cfg[\"emb_dim\"],\n",
    "#             context_length=cfg[\"context_length\"],\n",
    "#             num_heads=cfg[\"n_heads\"],\n",
    "#             dropout=cfg[\"drop_rate\"],\n",
    "#             qkv_bias=cfg[\"qkv_bias\"])\n",
    "#         self.ff = FeedForward(cfg)\n",
    "#         self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "#         self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "#         self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Shortcut connection for attention block\n",
    "#         shortcut = x\n",
    "#         x = self.norm1(x)\n",
    "#         x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n",
    "#         x = self.drop_shortcut(x)\n",
    "#         x = x + shortcut  # Add the original input back\n",
    "\n",
    "#         # Shortcut connection for feed-forward block\n",
    "#         shortcut = x\n",
    "#         x = self.norm2(x)\n",
    "#         x = self.ff(x)\n",
    "#         x = self.drop_shortcut(x)\n",
    "#         x = x + shortcut  # Add the original input back\n",
    "\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"]).to(torch_device)\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"]).to(torch_device)\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"]).to(torch_device)\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        ).to(torch_device)\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"]).to(torch_device)\n",
    "        # GPT2 uses tied weights for the embedding and output layers\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False).to(torch_device)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        in_idx = in_idx.to(torch_device)\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=torch_device))\n",
    "        x = self.drop_emb(tok_embeds + pos_embeds)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiHeadAttention(nn.Module):\n",
    "#     def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "#         super().__init__()\n",
    "#         assert d_out % num_heads == 0, \"d_out must be divisible by n_heads\"\n",
    "\n",
    "#         self.d_out = d_out\n",
    "#         self.num_heads = num_heads\n",
    "#         self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
    "\n",
    "#         self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "#         self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "#         self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "#         self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#         self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         b, num_tokens, d_in = x.shape\n",
    "\n",
    "#         keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
    "#         queries = self.W_query(x)\n",
    "#         values = self.W_value(x)\n",
    "\n",
    "#         # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "#         # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "#         keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "#         values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "#         queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "#         # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "#         keys = keys.transpose(1, 2)\n",
    "#         queries = queries.transpose(1, 2)\n",
    "#         values = values.transpose(1, 2)\n",
    "\n",
    "#         # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "#         attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "#         # Original mask truncated to the number of tokens and converted to boolean\n",
    "#         mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "#         # Use the mask to fill attention scores\n",
    "#         attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "#         attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "#         attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "#         # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "#         context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "#         # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "#         context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
    "#         context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "#         return context_vec\n",
    "\n",
    "\n",
    "# #####################################\n",
    "# #Chapter 4\n",
    "# #####################################\n",
    "# class LayerNorm(nn.Module):\n",
    "#     def __init__(self, emb_dim):\n",
    "#         super().__init__()\n",
    "#         self.eps = 1e-5\n",
    "#         self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "#         self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         mean = x.mean(dim=-1, keepdim=True)\n",
    "#         var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "#         norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "#         return self.scale * norm_x + self.shift\n",
    "\n",
    "\n",
    "# class GELU(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return 0.5 * x * (1 + torch.tanh(\n",
    "#             torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "#             (x + 0.044715 * torch.pow(x, 3))\n",
    "#         ))\n",
    "\n",
    "\n",
    "# class FeedForward(nn.Module):\n",
    "#     def __init__(self, cfg):\n",
    "#         super().__init__()\n",
    "#         self.layers = nn.Sequential(\n",
    "#             nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "#             GELU(),\n",
    "#             nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.layers(x)\n",
    "\n",
    "\n",
    "# class TransformerBlock(nn.Module):\n",
    "#     def __init__(self, cfg):\n",
    "#         super().__init__()\n",
    "#         self.att = MultiHeadAttention(\n",
    "#             d_in=cfg[\"emb_dim\"],\n",
    "#             d_out=cfg[\"emb_dim\"],\n",
    "#             context_length=cfg[\"context_length\"],\n",
    "#             num_heads=cfg[\"n_heads\"],\n",
    "#             dropout=cfg[\"drop_rate\"],\n",
    "#             qkv_bias=cfg[\"qkv_bias\"])\n",
    "#         self.ff = FeedForward(cfg)\n",
    "#         self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "#         self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "#         self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Shortcut connection for attention block\n",
    "#         shortcut = x\n",
    "#         x = self.norm1(x)\n",
    "#         x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n",
    "#         x = self.drop_shortcut(x)\n",
    "#         x = x + shortcut  # Add the original input back\n",
    "\n",
    "#         # Shortcut connection for feed-forward block\n",
    "#         shortcut = x\n",
    "#         x = self.norm2(x)\n",
    "#         x = self.ff(x)\n",
    "#         x = self.drop_shortcut(x)\n",
    "#         x = x + shortcut  # Add the original input back\n",
    "\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class GPTModel(nn.Module):\n",
    "#     def __init__(self, cfg):\n",
    "#         super().__init__()\n",
    "#         self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "#         self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "#         self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "#         self.trf_blocks = nn.Sequential(\n",
    "#             *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "#         self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "#         self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "#     def forward(self, in_idx):\n",
    "#         batch_size, seq_len = in_idx.shape\n",
    "#         tok_embeds = self.tok_emb(in_idx)\n",
    "#         pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "#         x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "#         x = self.drop_emb(x)\n",
    "#         x = self.trf_blocks(x)\n",
    "#         x = self.final_norm(x)\n",
    "#         logits = self.out_head(x)\n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, token_ids, max_new_tokens, context_size):\n",
    "    logits = None\n",
    "    for i in range(max_new_tokens):\n",
    "        context_token_ids = token_ids[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(context_token_ids)\n",
    "\n",
    "        logits = logits[:, -1, :]\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        token_id_next = torch.argmax(probas, dim=1, keepdim=True)  # Pure Greed\n",
    "        token_ids = torch.cat((token_ids, token_id_next), dim=1) \n",
    "\n",
    "    return token_ids\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special=set(['<|endoftext|>']))\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0).to(torch_device)\n",
    "    return encoded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logits, targets):\n",
    "    vocab_size = logits.shape[-1]\n",
    "    loss = nn.CrossEntropyLoss()(logits.view(-1, vocab_size), targets.view(-1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "#     def __init__(self, txt, tokenizer, max_length, stride):\n",
    "#         self.input_ids = []\n",
    "#         self.target_ids = []\n",
    "#         token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "#         token_ids = token_ids\n",
    "#         token_ids = torch.tensor(token_ids).to(torch_device)\n",
    "#         print(token_ids.shape)\n",
    "#         token_sequences = token_ids.unfold(0, max_length, stride)\n",
    "#         self.input_ids = token_sequences[:-1]\n",
    "#         self.target_ids = token_sequences[1:]\n",
    "\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "def create_dataloader_v1(\n",
    "        txt, batch_size=4, max_length=256, \n",
    "        stride=128, shuffle=True, drop_last=True,\n",
    "        num_workers=0,\n",
    "    ):\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    return DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device=torch_device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.view(-1, logits.shape[-1]), \n",
    "        target_batch.view(-1),\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(dataloader, model, device, num_batches=None):\n",
    "    total_loss = 0\n",
    "    if len(dataloader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(dataloader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(dataloader))\n",
    "    for i, (input_batch, target_batch) in enumerate(dataloader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_dataloader, test_dataloader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_dataloader, model, device, num_batches=eval_iter)\n",
    "        test_loss = calc_loss_loader(test_dataloader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, test_loss\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    token_ids = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(model, token_ids, 50, context_size)\n",
    "    decoded_text = tokenizer.decode(token_ids[0].tolist())\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(\n",
    "    model, train_dataloader, test_dataloader,\n",
    "    optimizer, device, num_epochs,\n",
    "    eval_freq, eval_iter, start_context, tokenizer\n",
    "):\n",
    "    train_losses, test_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            # print(input_batch)\n",
    "            # print(target_batch)\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            loss.backward()\n",
    "            # grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            # print(f\"Gradient norm: {grad_norm:.4f}\")\n",
    "            optimizer.step()\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step %eval_freq == 0:\n",
    "                train_loss, test_loss = evaluate_model(\n",
    "                    model, train_dataloader, test_dataloader, device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)-\n",
    "                test_losses.append(test_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(\n",
    "                    f\"Epoch {epoch + 1} (Step {global_step:06d}): \"\n",
    "                    f\"Train Loss: {train_loss:.3f}, \"\n",
    "                    f\"Val Loss: {test_loss:.3f}\"\n",
    "                )\n",
    "\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "    return train_losses, test_losses, track_tokens_seen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "train_ratio = 0.9\n",
    "split_idx = int(len(raw_text) * train_ratio)\n",
    "train_data = raw_text[:split_idx]\n",
    "test_data = raw_text[split_idx:]\n",
    "\n",
    "torch.manual_seed(123)\n",
    "train_dataloader = create_dataloader_v1(train_data, batch_size=2, max_length=GPT_CONFIG_124M[\"context_length\"], stride=GPT_CONFIG_124M[\"context_length\"], shuffle=True)\n",
    "test_dataloader = create_dataloader_v1(test_data, batch_size=2, max_length=GPT_CONFIG_124M[\"context_length\"], stride=GPT_CONFIG_124M[\"context_length\"], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data[:100], test_data[:100]\n",
    "# tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "# tokenizer.encode(train_data[:100], allowed_special={\"<|endoftext|>\"})\n",
    "# len(train_dataloader), len(test_dataloader)\n",
    "# input_sample, target_sample = next(iter(train_dataloader))\n",
    "# torch.sum(input_sample[1]), torch.sum(target_sample[1])\n",
    "# input_sample.shape, \n",
    "# input_sample[1],target_sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "gpt = GPTModel(GPT_CONFIG_124M).to(torch_device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    gpt.parameters(),\n",
    "    lr = 0.0004, weight_decay = 0.1\n",
    ")\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Step 000000): Train Loss: 9.817, Val Loss: 9.924\n",
      "Epoch 1 (Step 000005): Train Loss: 8.066, Val Loss: 8.332\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Epoch 2 (Step 000010): Train Loss: 6.619, Val Loss: 7.042\n",
      "Epoch 2 (Step 000015): Train Loss: 6.046, Val Loss: 6.596\n",
      "Every effort moves you, and,, and, and,,,,, and, and,,,,,,,,,,, and,, the,, the, and,, and,,, the, and,,,,,,\n",
      "Epoch 3 (Step 000020): Train Loss: 5.524, Val Loss: 6.508\n",
      "Epoch 3 (Step 000025): Train Loss: 5.369, Val Loss: 6.378\n",
      "Every effort moves you, and to the of the of the picture. Gis.                                     \n",
      "Epoch 4 (Step 000030): Train Loss: 4.830, Val Loss: 6.263\n",
      "Epoch 4 (Step 000035): Train Loss: 4.586, Val Loss: 6.285\n",
      "Every effort moves you of the \"I the picture.                    \"I\"I the picture\"I had the picture\"I the picture and I had been the picture of\n",
      "Epoch 5 (Step 000040): Train Loss: 3.880, Val Loss: 6.130\n",
      "Every effort moves you know he had been his pictures, and I felt it's by his last word.                   \"Oh, and he had been the end, and he had been\n",
      "Epoch 6 (Step 000045): Train Loss: 3.530, Val Loss: 6.183\n",
      "Epoch 6 (Step 000050): Train Loss: 2.960, Val Loss: 6.123\n",
      "Every effort moves you know it was his pictures--I glanced after him, I had the last word.        \"Oh, and I was his pictures--I looked.   \"I looked. \"I looked. \n",
      "Epoch 7 (Step 000055): Train Loss: 2.832, Val Loss: 6.150\n",
      "Epoch 7 (Step 000060): Train Loss: 2.104, Val Loss: 6.133\n",
      "Every effort moves you know the picture to me--I glanced after him, and Mrs.  \"I was no great, the fact, the fact that, the moment--as Jack himself, as his pictures--as of the picture--because he was a little\n",
      "Epoch 8 (Step 000065): Train Loss: 1.691, Val Loss: 6.186\n",
      "Epoch 8 (Step 000070): Train Loss: 1.391, Val Loss: 6.230\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the fact with a little: \"Yes--and by me to me to have to see a smile behind his close grayish beard--as if he had the donkey. \"There were days when I\n",
      "Epoch 9 (Step 000075): Train Loss: 1.059, Val Loss: 6.251\n",
      "Epoch 9 (Step 000080): Train Loss: 0.800, Val Loss: 6.278\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the fact with a laugh: \"Yes--and by me!\"  He laughed again, and threw back the window-curtains, I saw that, and down the room, and now\n",
      "Epoch 10 (Step 000085): Train Loss: 0.569, Val Loss: 6.373\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n"
     ]
    }
   ],
   "source": [
    "train_losses, test_losses, track_tokens_seen = train_model_simple(\n",
    "    gpt, train_dataloader, test_dataloader, optimizer, torch_device,\n",
    "    num_epochs, eval_freq=5, eval_iter=5, start_context=\"Every effort moves you\", tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, test_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, test_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, track_tokens_seen, train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_o): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jejllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

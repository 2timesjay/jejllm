{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    torch_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=torch_device)\n",
    "    print (x)\n",
    "else:\n",
    "    torch_device = torch.device(\"cpu\")\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 3\n",
    "CONTEXT_LENGTH = 4\n",
    "QKV_DIM = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings = torch.randn(8, CONTEXT_LENGTH, EMBEDDING_DIM).to(torch_device)/math.sqrt(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_q = torch.randn(EMBEDDING_DIM, QKV_DIM).to(torch_device)\n",
    "w_k = torch.randn(EMBEDDING_DIM, QKV_DIM).to(torch_device)\n",
    "w_v = torch.randn(EMBEDDING_DIM, QKV_DIM).to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = torch.matmul(input_embeddings, w_q)\n",
    "keys = torch.matmul(input_embeddings, w_k)\n",
    "attn_scores = torch.matmul(query, keys.transpose(1, 2))\n",
    "attn_weights = torch.softmax(attn_scores*(QKV_DIM**0.5), dim=-1)\n",
    "values = torch.matmul(input_embeddings, w_v)\n",
    "context = torch.matmul(attn_weights, values)  # Is this correct or should I transpose attn_weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4733,  2.4863,  1.9936, -1.4273],\n",
       "        [ 1.4557, -0.1330, -0.3731,  1.0159],\n",
       "        [ 1.4137, -1.4526, -1.3766,  1.5809],\n",
       "        [-0.5814,  0.0731,  0.1643, -0.4147]], device='mps:0')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0100, 0.6590, 0.3283, 0.0026],\n",
       "        [0.5821, 0.0616, 0.0438, 0.3125],\n",
       "        [0.4341, 0.0075, 0.0084, 0.5499],\n",
       "        [0.1305, 0.3294, 0.3748, 0.1652]], device='mps:0')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000]], device='mps:0')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights[0].sum(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1628, -0.2580],\n",
       "        [-0.2273,  0.0354],\n",
       "        [-0.1950,  0.1418],\n",
       "        [ 0.0869, -0.1119]], device='mps:0')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "EMBEDDING_DIM = 3\n",
    "CONTEXT_LENGTH = 4\n",
    "QKV_DIM = 2\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.w_q = torch.randn(d_in, d_out).to(torch_device)\n",
    "        self.w_k = torch.randn(d_in, d_out).to(torch_device)\n",
    "        self.w_v = torch.randn(d_in, d_out).to(torch_device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        query = torch.matmul(input_embeddings, self.w_q)\n",
    "        keys = torch.matmul(input_embeddings, self.w_k)\n",
    "        attn_scores = torch.matmul(query, keys.transpose(1, 2))\n",
    "        attn_weights = torch.softmax(attn_scores*(self.d_out**0.5), dim=-1)\n",
    "        values = torch.matmul(input_embeddings, self.w_v)\n",
    "        context = torch.matmul(attn_weights, values)  # Is this correct or should I transpose attn_weights?\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0253,  0.7391],\n",
       "        [ 0.0095,  0.6203],\n",
       "        [-0.0135,  0.6879],\n",
       "        [-0.2222,  1.0093]], device='mps:0')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "input_embeddings = torch.randn(8, CONTEXT_LENGTH, EMBEDDING_DIM).to(torch_device)/math.sqrt(EMBEDDING_DIM)\n",
    "sa_v1 = SelfAttention_v1(EMBEDDING_DIM, QKV_DIM)\n",
    "context = sa_v1(input_embeddings)\n",
    "context[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 3\n",
    "CONTEXT_LENGTH = 4\n",
    "QKV_DIM = 2\n",
    "\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.w_q = nn.Linear(d_in, d_out, bias=qkv_bias).to(torch_device)\n",
    "        self.w_k = nn.Linear(d_in, d_out, bias=qkv_bias).to(torch_device)\n",
    "        self.w_v = nn.Linear(d_in, d_out, bias=qkv_bias).to(torch_device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        query = self.w_q(input_embeddings)\n",
    "        keys = self.w_k(input_embeddings)\n",
    "        attn_scores = torch.matmul(query, keys.transpose(1, 2))\n",
    "        attn_weights = torch.softmax(attn_scores*(self.d_out**0.5), dim=-1)\n",
    "        values = self.w_v(input_embeddings)\n",
    "        context = torch.matmul(attn_weights, values)  # Is this correct or should I transpose attn_weights?\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0099, -0.0742],\n",
       "        [-0.0119, -0.0634],\n",
       "        [-0.0090, -0.0737],\n",
       "        [-0.0149, -0.0619]], device='mps:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa_v2 = SelfAttention_v2(EMBEDDING_DIM, QKV_DIM)\n",
    "context = sa_v2(input_embeddings)\n",
    "context[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0099, -0.0742],\n",
       "        [-0.0119, -0.0634],\n",
       "        [-0.0090, -0.0737],\n",
       "        [-0.0149, -0.0619]], device='mps:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa_v1.w_k = sa_v2.w_k.weight.T\n",
    "sa_v1.w_q = sa_v2.w_q.weight.T\n",
    "sa_v1.w_v = sa_v2.w_v.weight.T\n",
    "context = sa_v1(input_embeddings)\n",
    "context[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jejllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    torch_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=torch_device)\n",
    "    print (x)\n",
    "else:\n",
    "    torch_device = torch.device(\"cpu\")\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "EMBEDDING_DIM = 3\n",
    "CONTEXT_LENGTH = 4\n",
    "QKV_DIM = 2\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length=CONTEXT_LENGTH, dropout=0.5, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.w_q = nn.Linear(d_in, d_out, bias=qkv_bias).to(torch_device)\n",
    "        self.w_k = nn.Linear(d_in, d_out, bias=qkv_bias).to(torch_device)\n",
    "        self.w_v = nn.Linear(d_in, d_out, bias=qkv_bias).to(torch_device)\n",
    "        self.dropout = nn.Dropout(dropout).to(torch_device)\n",
    "        self.register_buffer(\n",
    "            'mask', \n",
    "            torch.triu(\n",
    "                torch.ones(context_length, context_length), \n",
    "                diagonal=1,\n",
    "            ).to(torch_device)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        num_tokens = x.shape[-2]\n",
    "        queries = self.w_q(x)\n",
    "        keys = self.w_k(x)\n",
    "        attn_scores = queries @ keys.transpose(-2, -1)\n",
    "        causal_attn_scores = attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        causal_attn_weights = torch.softmax(causal_attn_scores*(self.d_out**0.5), dim=-1)\n",
    "        causal_attn_weights = self.dropout(causal_attn_weights)\n",
    "        values = self.w_v(x)\n",
    "        context = causal_attn_weights @ values\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6832,  2.0395],\n",
       "        [-0.1236,  0.6861],\n",
       "        [-0.1971,  0.5884],\n",
       "        [ 0.0926, -0.1444]], device='mps:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csa = CausalSelfAttention(EMBEDDING_DIM, QKV_DIM)\n",
    "input_embeddings = torch.randn(8, CONTEXT_LENGTH, EMBEDDING_DIM).to(torch_device)/(EMBEDDING_DIM**0.5)\n",
    "context = csa(input_embeddings)\n",
    "context[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "EMBEDDING_DIM = 3\n",
    "CONTEXT_LENGTH = 4\n",
    "QKV_DIM = 2\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out//num_heads\n",
    "        self.w_q = nn.Linear(d_in, d_out, bias=qkv_bias).to(torch_device)\n",
    "        self.w_k = nn.Linear(d_in, d_out, bias=qkv_bias).to(torch_device)\n",
    "        self.w_v = nn.Linear(d_in, d_out, bias=qkv_bias).to(torch_device)\n",
    "        self.w_o = nn.Linear(d_out, d_out).to(torch_device)\n",
    "        self.dropout = nn.Dropout(dropout).to(torch_device)\n",
    "        self.register_buffer(\n",
    "            'mask', \n",
    "            torch.triu(\n",
    "                torch.ones(context_length, context_length), \n",
    "                diagonal=1,\n",
    "            ).to(torch_device)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.w_k(x)\n",
    "        queries = self.w_q(x)\n",
    "        values = self.w_v(x)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(-3, -2)\n",
    "        # TODO: KV Cache Optimization\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim).transpose(-3, -2)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim).transpose(-3, -2)\n",
    "        attn_scores = queries @ keys.transpose(-2, -1)\n",
    "        causal_attn_scores = attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        causal_attn_weights = torch.softmax(causal_attn_scores*(keys.shape[-1]**0.5), dim=-1)\n",
    "        causal_attn_weights = self.dropout(causal_attn_weights)\n",
    "        context = (causal_attn_weights @ values).transpose(-3, -2)\n",
    "        context = context.contiguous().view(b, num_tokens, self.num_heads*self.head_dim)\n",
    "        context = self.w_o(context)\n",
    "        return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MultiHeadAttention' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m CONTEXT_LENGTH \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m      3\u001b[0m QKV_DIM \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m----> 5\u001b[0m mha \u001b[38;5;241m=\u001b[39m \u001b[43mMultiHeadAttention\u001b[49m(EMBEDDING_DIM, QKV_DIM, CONTEXT_LENGTH, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      6\u001b[0m context \u001b[38;5;241m=\u001b[39m mha(input_embeddings)\n\u001b[1;32m      7\u001b[0m context\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MultiHeadAttention' is not defined"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 3\n",
    "CONTEXT_LENGTH = 4\n",
    "QKV_DIM = 4\n",
    "\n",
    "mha = MultiHeadAttention(EMBEDDING_DIM, QKV_DIM, CONTEXT_LENGTH, 0.5, 2)\n",
    "context = mha(input_embeddings)\n",
    "context.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-5):\n",
    "        super().__init__()\n",
    "        # self.eps = eps\n",
    "        # self.weight = nn.Parameter(torch.ones(d_model))\n",
    "        # self.bias = nn.Parameter(torch.zeros(d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # mean = x.mean(-1, keepdim=True)\n",
    "        # std = x.std(-1, keepdim=True)\n",
    "        # x = self.weight * (x - mean) / (std + self.eps) + self.bias\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # self.attn = MultiHeadAttention(config[\"emb_dim\"], config[\"emb_dim\"], config[\"context_length\"], config[\"drop_rate\"], config[\"n_heads\"], config[\"qkv_bias\"])\n",
    "        # self.ln1 = nn.LayerNorm(config[\"emb_dim\"]).to(torch_device)\n",
    "        # self.mlp = nn.Sequential(\n",
    "        #     nn.Linear(config[\"emb_dim\"], 4*config[\"emb_dim\"]),\n",
    "        #     nn.GELU(),\n",
    "        #     nn.Linear(4*config[\"emb_dim\"], config[\"emb_dim\"]),\n",
    "        #     nn.Dropout(config[\"drop_rate\"]),\n",
    "        # ).to(torch_device)\n",
    "        # self.ln2 = nn.LayerNorm(config[\"emb_dim\"]).to(torch_device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = x + self.attn(self.ln1(x))\n",
    "        # x = x + self.mlp(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"]).to(torch_device)\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"]).to(torch_device)\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"]).to(torch_device)\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        ).to(torch_device)\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"]).to(torch_device)\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False).to(torch_device)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        in_idx = in_idx.to(torch_device)\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=torch_device))\n",
    "        x = self.drop_emb(tok_embeds + pos_embeds)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  268,  3754,   438,  ...,   286,   262,  7877],\n",
      "        [  438,  2016,   257,  ...,  7877,    72,  3150],\n",
      "        [  257,   922,  5891,  ...,  3150,   339,  8104],\n",
      "        ...,\n",
      "        [  340,   373,   645,  ..., 37918,   411,   290],\n",
      "        [  645,  1049,  5975,  ...,   290,  8465,   286],\n",
      "        [ 5975,   284,   502,  ...,   286,   281, 33954]], device='mps:0'), tensor([[  438,  2016,   257,  ...,  7877,    72,  3150],\n",
      "        [  257,   922,  5891,  ...,  3150,   339,  8104],\n",
      "        [ 5891,  1576,   438,  ...,  8104,   866,  1973],\n",
      "        ...,\n",
      "        [  645,  1049,  5975,  ...,   290,  8465,   286],\n",
      "        [ 5975,   284,   502,  ...,   286,   281, 33954],\n",
      "        [  502,   284,  3285,  ..., 33954,   271,  3973]], device='mps:0')]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "        token_ids = token_ids\n",
    "        token_ids = torch.tensor(token_ids).to(torch_device)\n",
    "        token_sequences = token_ids.unfold(0, max_length, stride)\n",
    "        self.input_ids = token_sequences[:-1]\n",
    "        self.target_ids = token_sequences[1:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "def create_dataloader_v1(\n",
    "        txt, batch_size=4, max_length=256, \n",
    "        stride=128, shuffle=True, drop_last=True,\n",
    "        num_workers=0,\n",
    "    ):\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    return DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "    )\n",
    "\n",
    "with open(\"../../data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "dataloader = create_dataloader_v1(raw_text[50:], batch_size=8, max_length=GPT_CONFIG_124M[\"context_length\"], stride=2, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1262,  0.0891,  0.5763,  ...,  0.0204, -0.2858, -0.5722],\n",
       "         [-0.5513, -0.4048,  1.3528,  ...,  0.2848, -0.4136,  0.1375],\n",
       "         [ 0.7228,  0.1680, -0.3502,  ..., -1.0963,  0.0880,  1.0310],\n",
       "         ...,\n",
       "         [-0.3543, -1.2870,  0.5911,  ..., -0.2834, -0.1321,  0.1746],\n",
       "         [ 0.1739,  0.2213, -1.9957,  ...,  0.4184,  0.5599,  1.9302],\n",
       "         [ 0.0610,  0.2478,  1.1949,  ...,  0.4700,  1.1792, -0.6527]],\n",
       "\n",
       "        [[ 0.4774,  0.2370,  0.1052,  ...,  0.2165, -0.8574,  0.4955],\n",
       "         [-0.4754,  1.0396,  1.5438,  ...,  0.5923,  0.1351,  0.4887],\n",
       "         [ 0.9107, -0.1868, -0.3470,  ..., -0.5047, -1.2149,  0.1710],\n",
       "         ...,\n",
       "         [-0.8525, -1.3352,  1.8167,  ..., -0.4199,  1.2835,  0.0412],\n",
       "         [ 1.1149, -0.0914, -0.7332,  ..., -0.9261,  1.0550,  1.3011],\n",
       "         [ 1.9892, -0.7033,  0.0369,  ..., -0.2974, -0.0111,  0.1645]],\n",
       "\n",
       "        [[ 0.4093, -0.0050, -0.1816,  ...,  1.6179, -2.2207, -0.5701],\n",
       "         [-0.7639, -0.0053,  1.6686,  ...,  1.8961,  0.0654, -1.0891],\n",
       "         [ 0.2192,  0.3466, -0.5812,  ..., -1.0455,  0.1514,  0.3706],\n",
       "         ...,\n",
       "         [ 0.5987, -1.4804, -0.4095,  ..., -0.1126, -0.1053,  0.2662],\n",
       "         [ 1.5793, -0.6060, -0.9705,  ..., -0.5541,  1.3150,  0.9068],\n",
       "         [ 0.2933,  0.2093, -0.4258,  ..., -1.1677, -0.4003,  1.3701]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.3765, -0.2404, -0.2115,  ...,  0.2105,  0.3300,  0.2167],\n",
       "         [-1.8364,  0.6906,  0.4095,  ..., -0.1738,  0.2043, -0.0260],\n",
       "         [-0.6123, -0.5827, -0.9364,  ..., -0.8593, -0.3110,  0.4624],\n",
       "         ...,\n",
       "         [ 0.0250, -1.3587,  0.1008,  ..., -0.7093,  1.3264,  1.3922],\n",
       "         [ 1.0507, -0.3649, -1.8878,  ..., -0.7981,  0.6832,  1.4017],\n",
       "         [ 0.1665,  0.6924,  0.1439,  ...,  0.7476,  1.1833,  0.9231]],\n",
       "\n",
       "        [[-0.2671, -0.2984, -0.2742,  ...,  0.3586, -0.6360, -0.1993],\n",
       "         [-1.0681,  0.6388,  0.6155,  ..., -0.7540, -0.4628, -0.8418],\n",
       "         [-0.0262,  0.3079, -0.7613,  ..., -0.1014,  0.4698,  0.2049],\n",
       "         ...,\n",
       "         [-1.1534, -0.3179,  0.1459,  ...,  1.1102,  0.5474,  1.0167],\n",
       "         [ 1.2609, -0.8929, -1.2672,  ..., -0.5554,  1.6451,  0.5159],\n",
       "         [ 1.4930, -0.4392,  0.2566,  ..., -0.8386, -0.3983, -0.3885]],\n",
       "\n",
       "        [[-0.2074,  0.7390, -0.8953,  ...,  1.3175, -0.5195,  0.1961],\n",
       "         [ 0.3504,  0.1252,  2.3699,  ...,  0.7455,  0.0781, -0.8944],\n",
       "         [ 0.6012, -0.9864, -0.0188,  ..., -1.7677,  1.1171, -0.1010],\n",
       "         ...,\n",
       "         [ 0.2026, -1.5646,  0.2360,  ..., -0.8491,  0.5317, -0.0325],\n",
       "         [ 0.6989, -0.1215, -1.2046,  ...,  0.6016,  0.7762,  0.6470],\n",
       "         [-0.0759, -1.3015,  1.7532,  ..., -0.9642,  1.2870, -0.7172]]],\n",
       "       device='mps:0', grad_fn=<LinearBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt = DummyGPTModel(GPT_CONFIG_124M)\n",
    "gpt(first_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jejllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

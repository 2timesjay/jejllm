{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstrate torch + MPS (mac \"metal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    torch_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=torch_device)\n",
    "    print (x)\n",
    "else:\n",
    "    torch_device = torch.device(\"cpu\")\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark example\n",
    "https://pytorch.org/tutorials/recipes/recipes/benchmark.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def batched_dot_mul_sum(a, b):\n",
    "    '''Computes batched dot by multiplying and summing'''\n",
    "    return a.mul(b).sum(-1)\n",
    "\n",
    "\n",
    "def batched_dot_bmm(a, b):\n",
    "    '''Computes batched dot by reducing to ``bmm``'''\n",
    "    a = a.reshape(-1, 1, a.shape[-1])\n",
    "    b = b.reshape(-1, b.shape[-1], 1)\n",
    "    return torch.bmm(a, b).flatten(-3)\n",
    "\n",
    "\n",
    "# Input for benchmarking\n",
    "x = torch.randn(10000, 64, device=torch_device)\n",
    "\n",
    "# Ensure that both functions compute the same output\n",
    "assert batched_dot_mul_sum(x, x).allclose(batched_dot_bmm(x, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mul_sum(x, x):   63.0 us\n",
      "bmm(x, x):       23.1 us\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "t0 = timeit.Timer(\n",
    "    stmt='batched_dot_mul_sum(x, x)',\n",
    "    setup='from __main__ import batched_dot_mul_sum',\n",
    "    globals={'x': x})\n",
    "\n",
    "t1 = timeit.Timer(\n",
    "    stmt='batched_dot_bmm(x, x)',\n",
    "    setup='from __main__ import batched_dot_bmm',\n",
    "    globals={'x': x})\n",
    "\n",
    "print(f'mul_sum(x, x):  {t0.timeit(100) / 100 * 1e6:>5.1f} us')\n",
    "print(f'bmm(x, x):      {t1.timeit(100) / 100 * 1e6:>5.1f} us')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x136111ed0>\n",
      "batched_dot_mul_sum(x, x)\n",
      "setup: from __main__ import batched_dot_mul_sum\n",
      "  63.37 us\n",
      "  1 measurement, 100 runs , 1 thread\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x136111ed0>\n",
      "batched_dot_bmm(x, x)\n",
      "setup: from __main__ import batched_dot_bmm\n",
      "  21.67 us\n",
      "  1 measurement, 100 runs , 1 thread\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.benchmark as benchmark\n",
    "\n",
    "t0 = benchmark.Timer(\n",
    "    stmt='batched_dot_mul_sum(x, x)',\n",
    "    setup='from __main__ import batched_dot_mul_sum',\n",
    "    globals={'x': x})\n",
    "\n",
    "t1 = benchmark.Timer(\n",
    "    stmt='batched_dot_bmm(x, x)',\n",
    "    setup='from __main__ import batched_dot_bmm',\n",
    "    globals={'x': x})\n",
    "\n",
    "print(t0.timeit(100))\n",
    "print(t1.timeit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking on 8 threads\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x135e43610>\n",
      "Multithreaded batch dot: Implemented using mul and sum\n",
      "setup: from __main__ import batched_dot_mul_sum\n",
      "  183.99 us\n",
      "  1 measurement, 100 runs , 8 threads\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x136113710>\n",
      "Multithreaded batch dot: Implemented using bmm\n",
      "setup: from __main__ import batched_dot_bmm\n",
      "  19.22 us\n",
      "  1 measurement, 100 runs , 8 threads\n"
     ]
    }
   ],
   "source": [
    "num_threads = torch.get_num_threads()\n",
    "print(f'Benchmarking on {num_threads} threads')\n",
    "\n",
    "t0 = benchmark.Timer(\n",
    "    stmt='batched_dot_mul_sum(x, x)',\n",
    "    setup='from __main__ import batched_dot_mul_sum',\n",
    "    globals={'x': x},\n",
    "    num_threads=num_threads,\n",
    "    label='Multithreaded batch dot',\n",
    "    sub_label='Implemented using mul and sum')\n",
    "\n",
    "t1 = benchmark.Timer(\n",
    "    stmt='batched_dot_bmm(x, x)',\n",
    "    setup='from __main__ import batched_dot_bmm',\n",
    "    globals={'x': x},\n",
    "    num_threads=num_threads,\n",
    "    label='Multithreaded batch dot',\n",
    "    sub_label='Implemented using bmm')\n",
    "\n",
    "print(t0.timeit(100))\n",
    "print(t1.timeit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mul_sum(x, x):   67.7 us\n",
      "mul_sum(x, x):   66.3 us\n",
      "bmm(x, x):      100.6 us\n",
      "bmm(x, x):      134.5 us\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10000, 1024, device=torch_device)\n",
    "\n",
    "t0 = timeit.Timer(\n",
    "    stmt='batched_dot_mul_sum(x, x)',\n",
    "    setup='from __main__ import batched_dot_mul_sum',\n",
    "    globals={'x': x})\n",
    "\n",
    "t1 = timeit.Timer(\n",
    "    stmt='batched_dot_bmm(x, x)',\n",
    "    setup='from __main__ import batched_dot_bmm',\n",
    "    globals={'x': x})\n",
    "\n",
    "# Ran each twice to show difference before/after warm-up\n",
    "print(f'mul_sum(x, x):  {t0.timeit(100) / 100 * 1e6:>5.1f} us')\n",
    "print(f'mul_sum(x, x):  {t0.timeit(100) / 100 * 1e6:>5.1f} us')\n",
    "print(f'bmm(x, x):      {t1.timeit(100) / 100 * 1e6:>5.1f} us')\n",
    "print(f'bmm(x, x):      {t1.timeit(100) / 100 * 1e6:>5.1f} us')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 1\n",
      "1 1 4\n",
      "1 1 8\n",
      "1 1 32\n",
      "1 64 1\n",
      "1 64 4\n",
      "1 64 8\n",
      "1 64 32\n",
      "1 1024 1\n",
      "1 1024 4\n",
      "1 1024 8\n",
      "1 1024 32\n",
      "1 10000 1\n",
      "1 10000 4\n",
      "1 10000 8\n",
      "1 10000 32\n",
      "64 1 1\n",
      "64 1 4\n",
      "64 1 8\n",
      "64 1 32\n",
      "64 64 1\n",
      "64 64 4\n",
      "64 64 8\n",
      "64 64 32\n",
      "64 1024 1\n",
      "64 1024 4\n",
      "64 1024 8\n",
      "64 1024 32\n",
      "64 10000 1\n",
      "64 10000 4\n",
      "64 10000 8\n",
      "64 10000 32\n",
      "1024 1 1\n",
      "1024 1 4\n",
      "1024 1 8\n",
      "1024 1 32\n",
      "1024 64 1\n",
      "1024 64 4\n",
      "1024 64 8\n",
      "1024 64 32\n",
      "1024 1024 1\n",
      "1024 1024 4\n",
      "1024 1024 8\n",
      "1024 1024 32\n",
      "1024 10000 1\n",
      "1024 10000 4\n",
      "1024 10000 8\n",
      "1024 10000 32\n",
      "10000 1 1\n",
      "10000 1 4\n",
      "10000 1 8\n",
      "10000 1 32\n",
      "10000 64 1\n",
      "10000 64 4\n",
      "10000 64 8\n",
      "10000 64 32\n",
      "10000 1024 1\n",
      "10000 1024 4\n",
      "10000 1024 8\n",
      "10000 1024 32\n",
      "10000 10000 1\n",
      "10000 10000 4\n",
      "10000 10000 8\n",
      "10000 10000 32\n",
      "[-------------- Batched dot ---------------]\n",
      "                      |  mul/sum  |    bmm  \n",
      "1 threads: ---------------------------------\n",
      "      [1, 1]          |      2.0  |      3.3\n",
      "      [1, 64]         |      2.0  |      3.3\n",
      "      [1, 1024]       |      2.0  |      5.1\n",
      "      [1, 10000]      |      3.0  |     10.6\n",
      "      [64, 1]         |      2.1  |      3.4\n",
      "      [64, 64]        |      3.1  |      6.3\n",
      "      [64, 1024]      |     11.0  |    116.1\n",
      "      [64, 10000]     |    192.6  |    469.3\n",
      "      [1024, 1]       |      3.0  |      5.7\n",
      "      [1024, 64]      |     18.7  |     49.5\n",
      "      [1024, 1024]    |    300.0  |   1784.2\n",
      "      [1024, 10000]   |   4424.5  |   7167.4\n",
      "      [10000, 1]      |     12.9  |     27.3\n",
      "      [10000, 64]     |    303.2  |    445.5\n",
      "      [10000, 1024]   |   3933.7  |  17212.4\n",
      "      [10000, 10000]  |  45006.3  |  71432.2\n",
      "4 threads: ---------------------------------\n",
      "      [1, 1]          |      1.9  |      3.3\n",
      "      [1, 64]         |      2.0  |      3.3\n",
      "      [1, 1024]       |      2.0  |      5.1\n",
      "      [1, 10000]      |      3.0  |     10.6\n",
      "      [64, 1]         |      2.1  |      3.4\n",
      "      [64, 64]        |      3.1  |      6.4\n",
      "      [64, 1024]      |     10.8  |    116.2\n",
      "      [64, 10000]     |     95.4  |    473.4\n",
      "      [1024, 1]       |      3.0  |      5.7\n",
      "      [1024, 64]      |     12.3  |     28.3\n",
      "      [1024, 1024]    |    153.9  |   1797.9\n",
      "      [1024, 10000]   |   1742.0  |   7372.2\n",
      "      [10000, 1]      |     14.0  |     26.7\n",
      "      [10000, 64]     |    120.6  |    120.8\n",
      "      [10000, 1024]   |   1835.1  |  17189.7\n",
      "      [10000, 10000]  |  21580.1  |  70931.5\n",
      "8 threads: ---------------------------------\n",
      "      [1, 1]          |      1.9  |      3.3\n",
      "      [1, 64]         |      2.0  |      3.3\n",
      "      [1, 1024]       |      2.1  |      5.1\n",
      "      [1, 10000]      |      3.0  |     10.7\n",
      "      [64, 1]         |      2.1  |      3.4\n",
      "      [64, 64]        |      3.1  |      6.3\n",
      "      [64, 1024]      |     17.2  |    116.6\n",
      "      [64, 10000]     |    129.8  |    470.0\n",
      "      [1024, 1]       |      3.0  |      5.7\n",
      "      [1024, 64]      |     19.3  |     29.9\n",
      "      [1024, 1024]    |    174.9  |   1804.1\n",
      "      [1024, 10000]   |   1740.2  |   7225.3\n",
      "      [10000, 1]      |     13.6  |     26.8\n",
      "      [10000, 64]     |    152.1  |    132.4\n",
      "      [10000, 1024]   |   1832.0  |  17376.5\n",
      "      [10000, 10000]  |  19444.9  |  71702.1\n",
      "32 threads: --------------------------------\n",
      "      [1, 1]          |      2.0  |      3.3\n",
      "      [1, 64]         |      1.9  |      3.3\n",
      "      [1, 1024]       |      2.1  |      5.2\n",
      "      [1, 10000]      |      3.0  |     10.6\n",
      "      [64, 1]         |      2.1  |      3.4\n",
      "      [64, 64]        |      3.1  |      6.2\n",
      "      [64, 1024]      |    262.5  |    115.5\n",
      "      [64, 10000]     |    329.0  |    470.6\n",
      "      [1024, 1]       |      3.0  |      5.7\n",
      "      [1024, 64]      |    252.8  |    138.5\n",
      "      [1024, 1024]    |    404.9  |   1800.2\n",
      "      [1024, 10000]   |   2228.5  |   7361.3\n",
      "      [10000, 1]      |     12.9  |     27.2\n",
      "      [10000, 64]     |    501.1  |    232.9\n",
      "      [10000, 1024]   |   2646.4  |  17292.4\n",
      "      [10000, 10000]  |  19795.8  |  71349.4\n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "# Compare takes a list of measurements which we'll save in results.\n",
    "results = []\n",
    "\n",
    "sizes = [1, 64, 1024, 10000]\n",
    "for b, n in product(sizes, sizes):\n",
    "    # label and sub_label are the rows\n",
    "    # description is the column\n",
    "    label = 'Batched dot'\n",
    "    sub_label = f'[{b}, {n}]'\n",
    "    x = torch.ones((b, n))\n",
    "    for num_threads in [1, 4, 8, 32]:\n",
    "        print(b,n,num_threads)\n",
    "        results.append(benchmark.Timer(\n",
    "            stmt='batched_dot_mul_sum(x, x)',\n",
    "            setup='from __main__ import batched_dot_mul_sum',\n",
    "            globals={'x': x},\n",
    "            num_threads=num_threads,\n",
    "            label=label,\n",
    "            sub_label=sub_label,\n",
    "            description='mul/sum',\n",
    "        ).blocked_autorange(min_run_time=1))\n",
    "        results.append(benchmark.Timer(\n",
    "            stmt='batched_dot_bmm(x, x)',\n",
    "            setup='from __main__ import batched_dot_bmm',\n",
    "            globals={'x': x},\n",
    "            num_threads=num_threads,\n",
    "            label=label,\n",
    "            sub_label=sub_label,\n",
    "            description='bmm',\n",
    "        ).blocked_autorange(min_run_time=1))\n",
    "\n",
    "compare = benchmark.Compare(results)\n",
    "compare.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jejllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
